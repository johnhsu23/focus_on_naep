{"timeline": [{"code": "november2002", "date": "November 2002", "title": "National Commission on NAEP 12th-Grade Assessment and Reporting established", "category": "Commissions and Technical Panels", "tier1": "<p>At Executive Director Roy Truby's recommendation, the Governing Board established the National Commission on NAEP 12th-Grade Assessment and Reporting in 2002. The National Commission was charged with reviewing the 12th-grade assessment and developing recommendations for the Governing Board for improving it.</p><p>Low participation rates by schools and students in 2002 had raised serious concerns about the viability of 12th-grade NAEP. An important consideration by the National Commission was whether NAEP should continue to assess students at the 12th-grade level, and the conclusion was a strong affirmative.</p>", "tier2": "<p>The National Commission set forth recommendations for actions aimed at increasing interest in and the value of 12th-grade NAEP results. Reporting on the preparedness of 12th-graders for postsecondary education and work was seen as an important means for accomplishing that goal.</p>"},
{"code": "september2003", "date": "September 2003", "title": "Paper commissioned to evaluate the feasibility and usefulness of NAEP data on the preparedness of 12th-graders ", "category": "Efforts to Evaluate Feasibility of Reporting Preparedness", "tier1": "<p>Michael Kirst, an expert on education research and policy, was commissioned to write a paper to evaluate the feasibility and usefulness of NAEP data on the preparedness of 12th-graders. <a href='https://www.nagb.gov/content/nagb/assets/documents/naep/College-Preparation-and-Grade-12-NAEP.doc'>College Preparation and Grade 12 NAEP</a> was presented to the Governing Board in September 2003. The report pointed to the problems and challenges that this endeavor would face, but it was generally supportive of the goal (Kirst, 2003).</p>", "footnoteIndexes": [12]},
{"code": "march2004", "date": "March 2004", "title": "National Commission report delivered calling for a \"bold, new vision\" for 12th-grade NAEP", "category": "Commissions and Technical Panels", "tier1": "<p>The National Commission on NAEP 12th-Grade Assessment and Reporting delivered its report to the Governing Board in March 2004. The report, <a href='https://www.nagb.gov/focus-areas/reports/12th-grade-achievement-in-america.html'>12th Grade Student Achievement in America: A New Vision for NAEP</a>, called for a \"bold, new vision\" for 12th-grade NAEP (National Commission on NAEP 12th- Grade Assessment and Reporting, 2004, p. 2). A key recommendation in the report was for NAEP to report on the readiness of 12th-grade students for college, training for employment, and entrance into the military. The Governing Board accepted the National Commission's report and began considering the recommendations and how they could be implemented.</p>", "tier2": "<p>The National Commission affirmed that NAEP should continue to assess 12th-grade students, and they proposed ways to increase the viability and validity of the assessment.<p><ul><li>The recommendation given highest priority was to expand state NAEP assessments from grades 4 and 8 to include grade 12. The National Commission recommended that school participation in the 12th-grade state NAEP be mandatory for reading and mathematics and voluntary in science and writing. This would provide more useful information about student achievement and increase interest and participation in the assessment of 12th-graders.</li><li>Next in importance was the recommendation for NAEP to report on the readiness of 12th-grade students for college, training for employment, and entrance into the military.</li><li>Three additional recommendations included (a) development of innovative strategies for increasing participation of high schools and students in 12th-grade NAEP and for motivating students to perform to their highest ability on the assessment; (b) giving prominence to 12th-grade NAEP performance in NAEP reporting; and (c) continuing the 12th-grade NAEP transcript studies and enhancing them by expanding the information collected and improving the dissemination of results (National Commission on NAEP 12th-Grade Assessment and Reporting, 2004).</li><p>Each committee of the Governing Board was involved in developing and implementing one or more recommendations of the National Commission, and each committee began discussing and planning how best to organize efforts to achieve the goals.</p>", "footnoteIndexes": [36]},
{"code": "year2004", "date": "2004", "title": "Papers commissioned on issues related to validity and standards for college, workforce, and military preparedness", "category": "Efforts to Evaluate Feasibility of Reporting Preparedness", "tier1": "<p>Several persons with expertise in relevant areas were identified and commissioned to write papers on specific aspects of the recommendations from the National Commission on NAEP 12th-Grade Assessment and Reporting. Reports included:</p><ul><li><a href = \"https://www.nagb.gov/content/nagb/assets/documents/what-we-do/andy.doc\">Twelfth Grade NAEP as an Indicator of College Readiness: Validity Issues and Methodological Options</li><li><a href = \"https://www.nagb.gov/content/nagb/assets/documents/what-we-do/Alley.doc\">Twelfth Grade NAEP and Readiness for Entrance into the Military: Validity Issues and Methodological Options</a></li><li><a href=\"https://www.nagb.gov/content/nagb/assets/documents/what-we-do/laird.doc\">Standard-Setting on Admissions Tests in Higher Education: The Uses of Admissions and Placement Tests by Selected Groups of Two- and Four-Year Colleges and Universities</a></li><li><a href=\"https://www.nagb.gov/content/nagb/assets/documents/what-we-do/schmitt.doc\">Readiness for Training Standards: Development and Validation</a></li><li><a href=\"https://www.nagb.gov/content/nagb/assets/documents/what-we-do/sellman.doc\">Predicting Readiness for Military Service: How Enlistment Standards Are Established</a></li></ul>", "tier2": "<p>In <a href=\"https://www.nagb.gov/content/nagb/assets/documents/what-we-do/andy.doc\">Twelfth Grade NAEP as an Indicator of College Readiness: Validity Issues and Methodological Options</a>, Andrew Porter (2004) analyzed several issues related to the high school–college transition and ways to evaluate the validity of preparedness research. He recommended that a single measure of preparedness for college-level work was likely to be inappropriate due to the difference between open admissions and highly selective institutions. See research studies conducted in December 2005 and January 2006. These studies provided initial indication that a single (minimal) level of preparedness would be sufficient; it would not be necessary to identify levels of preparedness associated with college admission criteria.</p><p>William Alley (2005) outlined several alternative validity procedures in <a href=\"https://www.nagb.gov/content/nagb/assets/documents/what-we-do/Alley.doc\">Twelfth Grade NAEP and Readiness for Entrance into the Military: Validity Issues and Methodological Options</a>. He noted that it would be necessary to establish empirical linkages between performance on the NAEP assessments by 12th-graders and measures of success in the military. He proposed and evaluated longitudinal, concurrent, and synthetic validation designs as approaches for research to establish the feasibility of using NAEP performance to predict preparedness for military service.</p><p>In <a href=\"https://www.nagb.gov/content/nagb/assets/documents/what-we-do/laird.doc\">Standard-Setting on Admissions Tests in Higher Education: The Uses of Admissions and Placement Tests by Selected Groups of Two- and Four-Year Colleges and Universities</a>, Bob Laird (2004) analyzed the use of admissions and placement tests by two-year and four-year colleges and universities to determine the point at which individual colleges and universities deem entering students to be college-ready.</p><p>Neal Schmitt (2004) provided specific recommendations for how the Governing Board should develop measures of preparedness for workforce training. He noted in <a href=\"https://www.nagb.gov/content/nagb/assets/documents/what-we-do/schmitt.doc\">Readiness for Training Standards: Development and Validation</a> that preparedness for workforce training measures would present several challenges for NAEP, including assessing noncognitive measures, involving training staff as content experts, and assessing nonacademic knowledge, skills, and abilities, all of which would require significant changes to NAEP.</p><p>In <a href=\"https://www.nagb.gov/content/nagb/assets/documents/what-we-do/sellman.doc\">Predicting Readiness for Military Service: How Enlistment Standards Are Established</a>, W. S. Sellman (2004) described the Department of Defense enlistment testing program, specifically how applicants are screened and qualified for entrance and how new recruits are assigned to military occupations. He recommended that the Governing Board consider establishing empirical linkages between NAEP and the Armed Services Vocational Aptitude Battery (ASVAB) to establish readiness for military service.</p>"},
{"code": "february2005", "date": "February 2005", "title": "Achieve, Inc., Report: Evaluating the NAEP Reading Framework as a measure of preparedness", "category": "Efforts to Evaluate Feasibility of Reporting Preparedness", "tier1": "<p>Achieve, Inc., presented its report, Recommendations to the National Assessment Governing Board on Aligning 12th Grade NAEP With College and Workplace Expectations: Reading, to the Governing Board in February 2005.</p><p>The report recommended that NAEP move forward with plans for revising the NAEP Reading Framework to report the preparedness at the 12th-grade level for the 2009 assessment. It also recommended that a single definition be used for preparedness for all postsecondary activities—military and civilian careers and higher education (Achieve, Inc., 2005).</p><p>The recommendations by Achieve were considered by the Governing Board, chiefly through the Assessment Development Committee, and recommendations were later incorporated into the framework and test specifications for 12th-grade NAEP reading.</p>", "tier2": "<p>The Achieve report was commissioned by the Governing Board in fall 2004 to address the following questions:</p><ol><li>\"Is there a single concept of reading preparedness that can be productively defined for students entering college, the world of work, and the military? How can the report of the American Diploma Project inform the analysis of the 2009 NAEP [Reading] Framework, in terms of its content?</li><li>What changes, if any, should be made to the cognitive targets and item construction, to enable reporting preparedness for college, training for employment, and entrance into the military?</li><li>What changes should be considered for the achievement-level descriptions in reading to enable reporting of student preparedness?\" (Achieve, Inc., 2005, p. 1).</li></ol><p>Achieve recommended that a single definition be used for preparedness for all postsecondary activities—military and civilian careers and higher education. The report specified that readiness for careers should be assumed to be in \"high trajectory\" careers—careers in high-growth areas that offer a solid wage and place the employee on a path to advancement. Achieve contended that a national consensus was developing with an understanding that the requirements for successful careers—civilian or military—were the same as the academic requirements for placement in college credit-bearing courses.</p><p>Modifications to the achievement-level descriptions were also recommended to ensure that they more explicitly reflected the content objectives of the frameworks to include passage complexity, content expectations, and cognitive targets. Further descriptors would need to reflect literacy skills needed for reporting workplace and military preparedness.</p><p>In addition, the Achieve report provided a list of several studies that could potentially be implemented for studying the validity of using 12th-grade NAEP as a measure of preparedness for college as well as civilian and military careers.</p>", "footnoteIndexes": [0]},
{"code": "january2005-march2005", "date": "January 2005–March 2005", "title": "NAEP Assessments Conducted", "category": "Governing Board Actions and Decisions", "tier1": "<p>Nine hundred schools and 21,000 students were assessed in the grade 12 NAEP assessments, including approximately 12,000 students who were assessed in reading and approximately 9,000 who were assessed in mathematics.</p>"},
{"code": "may2005", "date": "May 2005", "title": "Plans approved to assess and report on 12th-grade preparedness", "category": "Governing Board Actions and Decisions", "tier1": "<p>At the May 2005 Governing Board meeting, the Governing Board unanimously voted to go forward with plans for assessing and reporting on preparedness of 12th-grade students for postsecondary activities, including college, work, and military service. The Governing Board passed a <a href='https://www.nagb.gov/content/nagb/assets/documents/policies/resolution-on-preparedness.pdf'>resolution</a> to guide their future preparedness work (National Assessment Governing Board, 2005).</p>", "tier2": "<p>At the May 2005 Governing Board meeting, committees discussed the issues identified to date and began to develop plans for addressing them. The Committee on Standards, Design and Methodology (COSDAM) generally preferred to report a single measure of postsecondary preparedness—for college and career—but concluded that it would be necessary to report preparedness for each activity. They stipulated that reporting preparedness presumed that 12th-grade NAEP for reading and mathematics would be modified to be appropriate measures of preparedness.</p><p>The following provisions were included in the resolution passed by the Governing Board:</p><ol><li>\"NAEP will pursue assessment and reporting on 12th grade student achievement as it relates to preparedness for postsecondary pursuits, such as college-credit course work, training for employment, and entrance into the military, as measured by an assessment consistent with that purpose;</li><li>The working definition for preparedness shall be based on the conception of a high school graduate who does not require remediation in the subject being tested by NAEP in qualifying for college-level courses, training for employment, or entrance into the military;</li><li>The Governing Board shall (a) begin by revising assessment frameworks and developing performance standards in reading and mathematics in a manner designed to enable such reporting of preparedness; and, subsequently, (b) conduct studies to obtain evidence to support the validity of statements about preparedness intended for reporting by NAEP; and</li><li>The Governing Board shall determine which other subjects assessed by NAEP should be designated to report on preparedness and the sequence and timing for developing associated assessment frameworks and performance standards\" (National Assessment Governing Board, 2005, p. 1).</li></ol>", "footnoteIndexes": [15]},
{"code": "november2005", "date": "November 2005", "title": "Ad Hoc Committee to Plan for 12th-Grade NAEP in 2009 appointed", "category": "Governing Board Actions and Decisions", "tier1": "<p>In November 2005, the Governing Board created the <a href='https://www.nagb.gov/content/dam/nagb/en/documents/policies/The%20Future%20of%2012th%20Grade%20NAEP.pdf'>Ad Hoc Committee on Planning for NAEP 12th-Grade Assessments in 2009</a>. The committee was made up of Governing Board members and charged with addressing three specific areas:</p><ol><li>Conducting 12th-grade assessments at the state level</li><li>Reporting on the preparedness of 12th-grade students for college-credit coursework, training for employment, and entrance into the military</li><li>Improving 12th-grade school and student participation (National Assessment Governing Board, 2006)</li></ol>", "footnoteIndexes": [16]},
{"code": "december2005-and-january2006", "date": "December 2005 and January 2006", "title": "Panels conducted with higher education representatives and domestic/international employers", "category": "Efforts to Evaluate Feasibility of Reporting Preparedness", "tier1": "<p>Preparedness panels were convened by the Governing Board to collect information from representatives of higher education (December 2005) and the workplace (January 2006) to gather judgments from individuals who work directly with the students who complete 12th grade and take the next step into college or a career. This information was intended to help guide the deliberations by the Governing Board and to shape the study designs for NAEP preparedness research.</p><p>Among the issues the panel was whether preparedness should be considered as a dichotomous variable (prepared versus not prepared) or as having multiple levels (less prepared, prepared, highly prepared, and so forth). Both panels ultimately favored reporting preparedness as a dichotomous variable (i.e., one is either prepared or not prepared for the postsecondary activity). The level of preparedness expected in reading was not considered to be the same for college and work postsecondary activities.</p>", "tier2": "<p>Panelists for the college preparedness study were selected to be representative according to the enrollment size of their institution, the selectivity of their institution (highly selective, selective, and open enrollment), and by the type of institution (two year or four year). Geographic and gender representation were also criteria for the selection of panelists. Similarly, companies in the workplace preparedness study represented a broad array of domestic and international employers from different sectors of the economy.</p><p>Tasks for each group included the following:</p><ul><li>Discuss how preparedness in reading is defined and evaluated by their institution or workplace.</li><li>Develop an operational definition of preparedness for higher education or the workplace.</li><li>Make a recommendation about the number of preparedness levels that would be appropriate for higher education or the workplace, what those levels should be called, and how those levels should be defined.</li></ul><p>Key findings of the panels included the following:</p><ul><li>The education group considered the gradations in preparedness levels to be associated with the level of admissions selectivity of institutions. They suggested that preparedness for college credit- bearing coursework generally requires the student to be at or above the <em>NAEP Proficient</em> achievement level for the reading assessment.</li><li>The workplace group argued that \"partial\" should be removed from the definition of the <em>NAEP Basic</em> achievement level, but they agreed that students meeting the basic level of achievement for reading could be prepared for the workplace.</li><ul>"},
{"code": "march2006", "date": "March 2006", "title": "Achieve, Inc., Report: Evaluating the NAEP Mathematics Framework as a measure of preparedness", "category": "Efforts to Evaluate Feasibility of Reporting Preparedness", "tier1": "<p>Achieve, Inc., presented its report, <a href='https://www.nagb.gov/content/nagb/assets/documents/commission/researchandresources/Achieve-Mathematics-Report.pdf'>Recommendations to the National Assessment Governing Board on Aligning the 12th Grade NAEP with College, Workplace, and Military Expectations: Mathematics</a>, to the Governing Board in March 2006.</p><p>The report recommended that workplace preparedness be defined such that the level of preparedness would be appropriate to \"high quality\" career options (Achieve, Inc., 2006, p. 2). Preparedness requirements for college and the workplace were considered the same. Achieve also recommended that a single measure of preparedness for the three postsecondary areas be reported and that achievement-level descriptions be developed to set cut points for each of the three types of postsecondary pathways (or areas) on each of the five NAEP mathematics scales.</p><p>The Achieve report recommendations were considered by the Governing Board, primarily by the Assessment Development Committee, and the 12th-grade NAEP Mathematics Framework and test specifications were modified to incorporate these recommendations.</p>", "tier2": "<p>The study followed the same design used by Achieve for the analysis of the 12th-grade NAEP Reading Framework reported in February 2005. The Achieve study focused on four questions and identified eight issues for which the authors provided recommendations for resolution. Questions were as follows:</p><ol><li>How should NAEP define student preparedness for college, training for employment, and entrance into the military?</li><li>What changes should be considered for the 2005 NAEP Mathematics Framework to reflect the expectations of Achieve's American Diploma Project?</li><li>What changes should NAEP consider for item construction, content balance, and cognitive targets to better reflect postsecondary college and career demands?</li><li>What changes should be considered to NAEP's achievement-level descriptions to enable meaningful reporting of student preparedness?\" (Achieve, Inc., 2006, p. 3).</li></ol>", "footnoteIndexes": [1]},
{"code": "august2006", "date": "August 2006", "title": "Ad hoc committee's recommendations approved for reporting 12th-grade preparedness", "category": "Governing Board Actions and Decisions", "tier1": "<p>The Ad Hoc Committee on Planning for NAEP 12th-Grade Assessments in 2009 was asked to provide recommendations to the Governing Board regarding three policy areas:</p><ul><li>conducting assessments at the state level in 12th grade;</li><li>reporting on 12th-grade student preparedness for college credit coursework, training for employment, and entrance into the military; and</li><li>improving 12th-grade school and student participation.</li></ul><p>The Committee produced a total of 11 recommendations covering the three postsecondary areas that the Governing Board had identified for preparedness reporting (e.g., readiness of 12th-grade students for college, training for employment, and entrance into the military) and delivered their <a href='https://www.nagb.gov/content/nagb/assets/documents/policies/The%20Future%20of%2012th%20Grade%20NAEP.pdf'>report</a> to the Governing Board in August 2006 (National Assessment Governing Board, 2006).</p><p>Seven recommendations pertained to the Governing Board's preparedness initiative and included modifying the targets for preparedness reporting to be sharper and more clearly defined. The Committee also recommended that plans for a state-level 12th-grade NAEP go forward to produce at least a pilot study of ten states.</p><p>These recommendations were officially accepted and approved in the August 2006 meeting of the Governing Board, and activities began to incorporate the recommendations into planning and implementing preparedness research for reporting in 2009.</p>", "tier2": "<p>Specifically, to define \"preparedness\" more clearly, the Committee recommended that it be limited to postsecondary education and training for occupations, including both civilian and military jobs. Preparedness would include training for military occupations and would eliminate reference to entry into the military. Further, preparedness should focus on academic qualifications and not involve noncognitive, nonacademic, or personal attributes.</p><p>The Committee also said that reporting on preparedness should be in accordance with existing NAEP achievement-level definitions rather than based on any separately developed preparedness performance standards. Additional recommendations regarding preparedness assessment and reporting were included.</p>", "footnoteIndexes": [16]},
{"code": "november2006", "date": "November 2006", "title": "Technical Panel on 12th- Grade Preparedness Research appointed", "category": "Commissions and Technical Panels", "tier1": "<p>The Governing Board agreed to appoint a panel of experts to assist the Governing Board staff with developing plans for research and validity studies appropriate for reporting preparedness of 12th-graders for postsecondary education and job training based on their performance on NAEP.</p><p>The Technical Panel concluded early on that the potential for such feasibility seemed sufficient to warrant going forward with the research.</p>", "tier2": "<p>A total of seven experts were identified for the Technical Panel on 12th-Grade Preparedness Research (Technical Panel). The membership represented expertise in psychometrics, validity research, the civilian workplace, military training, industrial and organizational psychology, college admissions requirements, and the overall transition from high school to college. Some had experience with NAEP, and some did not.</p><p>The independent panel met in person six times and had three teleconference meetings between June 2007 and September 2008. The chair of the panel, Michael Kirst, met with the Governing Board three times to provide information and updates, as well as to collect feedback from the full Governing Board. The Committee on Standards, Design and Methodology (COSDAM) was given oversight of the preparedness research, and detailed technical reports were provided to the committee at each quarterly Governing Board meeting throughout the tenure of the Technical Panel.</p><p>Recommendations that had been gathered prior to convening the Technical Panel were considered carefully as part of their intense and extensive deliberations.</p>"},
{"code": "january2008-february2008", "date": "January 2008–February 2008", "title": "Preliminary research conducted on comparability of 12th-grade NAEP and SAT assessments", "category": "Efforts to Evaluate Feasibility of Reporting Preparedness", "tier1": "<p>With the permission of the College Board, preliminary feasibility studies were conducted to evaluate the NAEP frameworks relative to the SAT for each subject, first for reading in January 2008 and then for mathematics in February 2008. Results indicated that it was feasible to move forward with plans for content alignment studies.</p>", "tier2": "<p>The Technical Panel on 12th-Grade Preparedness Research (Technical Panel) had identified several assessments to be used in statistical linking studies for producing preparedness reference points on the NAEP scale. Before undertaking the statistical linking studies, however, the content alignment between each assessment and NAEP was to be evaluated for confirmation of sufficient content alignment to warrant exploration of statistical linkages.</p><p>The Technical Panel recommended that a comparability study be conducted for both mathematics and reading to assess the feasibility of content alignment studies for NAEP. The content alignment studies were the foundation of the research agenda developed by the Technical Panel. The Technical Panel recognized that the content and test specifications for NAEP differed from the assessments to which NAEP was to be related, according to the research plans.</p>"},
{"code": "june2008", "date": "June 2008", "title": "Content alignment study plans reviewed by expert panel", "category": "Commissions and Technical Panels", "tier1": "<p>As recommended by the Technical Panel on 12th-Grade Preparedness Research, an expert content alignment group was convened to review plans and make recommendations for content alignment studies. The alignment studies were to be between NAEP and other assessments rather than between assessment and standards for course content and instruction.</p><p>The Webb methodology had been recommended by the Technical Panel. Dr. Norman Webb agreed that the following recommendations by the Technical Panel should be incorporated in the design for the studies and also agreed to develop a detailed design document to be used for multiple alignment studies:</p><ul><li>The studies should be bi-directional.</li><li>The studies should be conducted by a third party.</li><li>A pilot study in at least one of the two subjects for each assessment should be used in the research studies. The number of panelists and their qualifications were to be specified in the design as well as in the draft agenda.</li><li>The design document developed by Dr. Webb was approved by the Committee on Standards, Design and Methodology.</li></ul>", "tier2": "<p>Discussion topics and recommendations included the following:</p><ol><li>Unit of analysis (the key issue to resolve): the higher the level of comparison, the greater the probability of finding \"alignment\"</li><li>Qualifications and eligibility of panelists: high school and college subject-matter experts (half alignment experts and half subject-matter experts)</li><li>Materials to provide to panelists</li><li>Logistics: time for training and alignment tasks</li><li>Evidence that both alignment and nonalignment are presented as acceptable outcomes of the study</li><li>Decision rule: the alignment will be continuous—a matter of degree—because NAEP is highly visible and requires a higher percentage alignment than is typical.</li><li>Instructing panelists and maintaining common and consistent calibration: reach agreement among panelists on terminology before beginning alignment task; conduct intrarater and interrater consistency checks during study (National Assessment Governing Board, 2008).</li></ol>", "footnoteIndexes": [17]},
{"code": "november2008", "date": "November 2008", "title": "Technical Panel report providing a definition of preparedness and outlining the NAEP preparedness research agenda delivered", "category": "Efforts to Evaluate Feasibility of Reporting Preparedness", "tier1": "<p><a href='https://www.nagb.gov/content/nagb/assets/documents/commission/symposia/making-new-links.pdf'>Making New Links: 12th Grade and Beyond</a>, the final report of the Technical Panel on 12th-Grade Preparedness Research (Technical Panel), was delivered to the Governing Board in November 2008.</p><p>One of the policy decisions for the Technical Panel was to recommend  the feasibility of reporting statements about preparedness of 12th-graders based on NAEP results. They recognized that the operational definition of preparedness would be evolving through the research process. The Technical Panel recommended that the following working definition be used for conducting the NAEP preparedness research: \"Preparedness represents the academic knowledge and skill levels in reading and mathematics necessary to be qualified for placement into a job training program … or into a credit-bearing entry-level general education course that fulfills requirements toward a two-year transfer degree or four-year undergraduate degree at a postsecondary institution.\" See the March 2009 entry for the definition of preparedness adopted by the Governing Board as part of the recommendations for preparedness research presented by the Technical Panel (Technical Panel on 12th-Grade Preparedness Research, 2008).</p><p>In addition to providing a definition of preparedness for use in 12th-grade NAEP assessment development and reporting, the Technical Panel (2008) outlined a full research agenda, including studies related to content alignment, statistical relationships with other assessments and postsecondary outcomes data, criterion-based judgmental standard setting, and national surveys.</p><p>Recommendations of the Technical Panel were approved by the Governing Board in March 2009 for implementation in the preparedness research program.</p>", "tier2": "<p>The Technical Panel (2008) developed a definition of preparedness for the NAEP context that distinguished preparedness from readiness and preparedness for postsecondary education from preparedness for postsecondary job training. They considered that the academic preparedness requirements for civilian and military job training programs would be highly similar, if not equivalent.</p><p>The research plan that the Technical Panel laid out involved a multimethod approach that included four study designs: content alignment, statistical relationships with other assessments and postsecondary outcomes data, criterion-based judgmental standard setting, and national surveys. A model of the interrelationships among the studies was developed, and models of the pattern of potential findings across the studies were developed to indicate the potential for reporting preparedness based on student performance on NAEP. A key recommendation was that the results across studies would be evaluated to determine the extent to which the compilation of study results provided evidence for reporting preparedness on NAEP. No single study could be considered to provide a definitive indicator of preparedness to report on the NAEP scale.</p><p>Extensive deliberations resulted in detailed information regarding the questions to be answered by the different studies and the issues to be addressed. A timeline for the implementation of the studies, based on their logical sequencing, was developed in order to complete the research in time for reporting the results of the 2009 12th-grade NAEP assessments in reading and mathematics. In addition, the Technical Panel not only recommended that a set of future research studies be conducted but also that a technical panel be appointed to oversee and guide the research studies as they were implemented.</p>", "footnoteIndexes": [44]},
{"code": "december2008", "date": "December 2008", "title": "Statistical linking design finalized", "category": "Efforts to Produce Reference Points for Reporting Preparedness", "tier1": "<p>Draft versions of the plan for the design of the NAEP-SAT linking study were shared with the National Center for Education Statistics (NCES) in February and April 2008 for approval. The plan that was approved in December 2008 allowed for matching records without revealing the identity of students during matching and for the records to be matched by using pseudo-identification codes. This was an important procedure that won agreement to use data from other assessments for the statistical linking studies in the NAEP preparedness research program.</p>"},
{"code": "january2009", "date": "January 2009", "title": "Design documents finalized for content alignment studies", "category": "Efforts to Evaluate Feasibility of Reporting Preparedness", "tier1": "<p>The draft design document, <a href='https://www.nagb.gov/focus-areas/reports/design-content-alignment-studies.html'>Design of Content Alignment Studies in Mathematics and Reading for 12th Grade NAEP and other Assessments to be used in Preparedness Research Studies</a>, by Norman Webb (2009) was reviewed by the Committee on Standards, Design and Methodology (COSDAM); members of the Technical Panel; members of the expert content alignment group; and others with expertise in content alignment and NAEP frameworks. The document was then finalized and used to conduct the studies.</p>"},
{"code": "january2009-march2009", "date": "January 2009–March 2009", "title": "NAEP assessments conducted, including representative samples for 11 states to receive pilot 12th- grade results", "category": "Governing Board Actions and Decisions", "tier1": "<p>The first assessment of grade 12 NAEP to include state-representative samples was administered. A total of 11 states participated in the pilot state assessment for grade 12 in 2009. The entire grade 12 sample included 1,670 schools with approximately 52,000 12th-graders assessed in reading and 49,000 in mathematics.</p>"},
{"code": "march2009", "date": "March 2009", "title": "Preparedness research program approved for implementation", "category": "Governing Board Actions and Decisions", "tier1": "<p>The program of preparedness research developed by the Technical Panel on 12th-Grade Preparedness Research (Technical Panel) was approved by the Governing Board for implementation. Included in this approval were definitions of preparedness for college and workplace training. The Committee on Standards, Design and Methodology (COSDAM) approved the design for implementation of the content alignment studies as well as the appointment of a technical advisory panel to assist staff throughout implementation of preparedness research studies.</p><p>At the March 6, 2009 meeting, COSDAM approved the following definitions of preparedness for 12th-grade NAEP:</p><ul><li>\"Preparedness for college refers to the reading and mathematics knowledge and skills necessary to qualify for placement into entry level college credit courses that meet the general education requirements without the need for remedial coursework in mathematics or reading.</li><li>Preparedness for workplace refers to the reading and mathematics knowledge and skills needed to qualify for an occupation's job training program; it does not necessarily mean that the qualifications to be hired for a job have been met\" (National Assessment Governing Board, 2009).</li></ul>", "tier2": "<p>The report of the Technical Panel on 12th-Grade Preparedness Research, accepted by the Governing Board in November 2008, guided the 12th-grade NAEP preparedness research for all studies conducted for reporting the 2009 NAEP results for 12th-grade reading and mathematics.</p><p>COSDAM approved the design document for implementation of the alignment studies between 12th-grade NAEP assessments of reading and mathematics and ACCUPLACER, the SAT, ACT, and WorkKeys.</p><p>COSDAM also approved the recommendation to appoint a technical advisory panel consisting of five to seven members to advise staff throughout implementation of preparedness research studies. The technical advisors were to represent expertise in the types of studies recommended for the 12th-grade preparedness research, including alignment, judgmental standard setting, statistical linking, and survey research. The technical advisors were to begin service by May 1, 2009, and continue through September 2010, with the possibility of extension upon the mutual agreement of the Governing Board and individual advisors (National Assessment Governing Board, 2009).</p>", "footnoteIndexes": [18]},
{"code": "july2009-november2009", "date": "July 2009–November 2009", "title": "Content alignment studies conducted for NAEP and the ACT", "category": "Efforts to Evaluate Feasibility of Reporting Preparedness", "tier1": "<p>ACT, Inc., evaluated the content comparability between the NAEP assessments in reading and mathematics for 12th-grade and the ACT College Readiness Standards and Benchmarks in reading and mathematics. The studies for the two subjects were conducted in July 2009. ACT (2009) presented the final report <a href='https://www.nagb.gov/focus-areas/reports/preparedness-research/docs/content-alignment/landing/ACT-NAEP_Math_and_Reading_Content_Comparison.html'>NAEP in Relation to the ACT College Readiness Standards and Benchmarks in Reading and Mathematics</a> to the Governing Board on November 3, 2009.</p><p>The study did not use the design previously developed and approved by the Governing Board for the content alignment studies. ACT did not agree to the detailed analysis called for in that design, so a more general \"comparability\" study was conducted.</p><p>The study found considerable overlap in what ACT and NAEP assess in reading and mathematics. However, the report noted that there are important differences in what skills the assessments measure and how those skills are measured—warranting caution for direct comparisons of performance on the assessments (ACT, Inc., 2009).</p>", "tier2": "<p>The study had two major components. The first was a detailed comparison of the NAEP 12th-grade framework in the subject with the ACT content and cognitive domains, test specifications, and the ACT College Readiness Standards in the subject. The second, called the Item Classification Study, attempted to categorize each item in the NAEP assessment in reading or mathematics according to the ACT College Readiness Standards score ranges. Note that no items from the ACT assessments were used in the studies. The data were collected through the work of a panel of subject-matter experts that had been selected and convened by the ACT.</p><p><strong>Key Conclusions of the Reading Study for the ACT and 12th-Grade NAEP</strong></p><p>The panel found considerable overlap in what ACT and NAEP assess in reading. All of the cognitive skills in the ACT Reading Test and all of the skills included in the ACT College Readiness Standards were found to be present in the 12th-grade NAEP reading assessment. The ACT assessment includes only multiple-choice items, whereas NAEP includes a large number of questions requiring either short written responses or extended written responses.</p><p>This difference was considered a reason for why there were fewer higher-level analytical and evaluative skills required in the ACT reading assessment than in NAEP. Further, NAEP includes items that assess how well students apply reading skills across texts, whereas ACT does not assess that skill.</p><p>While the 12th-grade NAEP reading assessment and the ACT reading assessment share similarities, important differences in what reading skills the assessments measure—and how they measure those skills—warrant caution for direct comparisons of performance on the two assessments (National Assessment Governing Board, 2010).</p><p><strong>Key Conclusions of the Mathematics Study for the ACT and 12th-Grade NAEP</strong></p><p>The study found considerable overlap in what ACT and NAEP assess in mathematics:</p><ul><li>Although ACT reported considerable overlap between the two assessments in mathematics, some skills specified for measurement in the ACT mathematics assessment were a better fit with the 8th-grade NAEP mathematics assessment than with the 12th-grade NAEP mathematics assessment.</li><li>Almost all of the ACT content domains were covered by the NAEP mathematics assessment.</li><li>All of the skills represented in the ACT College Readiness Standards are measured on the NAEP mathematics assessment.</li></ul><p>The findings point to differences in what the ACT and NAEP assess in mathematics:</p><ul><li>In the NAEP-to-ACT comparison, the panel was uncertain about whether several topics assessed by NAEP—transformations, probability, statistics, and data analysis—were assessed by the ACT, because the ACT documents were not sufficiently specific to discern this.</li><li>The 12th-grade NAEP Mathematics Framework calls for items to measure higher-order analytical skills with mathematical complexity, but the ACT mathematics assessment does not.</li><li>While the 12th-grade NAEP mathematics assessment and the ACT mathematics assessment share similarities, there are important differences in what mathematics skills the assessments measure—and how they measure those skills—warrant caution for direct comparisons of performance on the two assessments (National Assessment Governing Board, 2010).</li></ul>", "footnoteIndexes": [2, 19]},
{"code": "december2009", "date": "December 2009", "title": "Pilot content alignment study conducted for NAEP and ACCUPLACER", "category": "Efforts to Evaluate Feasibility of Reporting Preparedness", "tier1": "<p>The pilot content alignment study with NAEP and ACCUPLACER was conducted for reading in December 2009. The final report was presented to the Governing Board in March 2010.</p><p>The operational studies are reported in the entry for March 2010–November 2010.</p>", "tier2": "<p>ACCUPLACER was selected for the pilot study because the NAEP and ACCUPLACER assessments were most likely to be different and because ACCUPLACER's alignment with the NAEP reading assessment was considered to provide the greatest challenges for alignment. Results indicated that the procedures in the design document worked―but not without some difficulty and not without some modifications for implementation in the operational studies.</p>"},
{"code": "january2010-october2010", "date": "January 2010–October 2010", "title": "Content alignment studies conducted for NAEP and WorkKeys", "category": "Efforts to Evaluate Feasibility of Reporting Preparedness", "tier1": "<p>ACT, Inc., conducted content alignment studies to analyze the content comparability between the 12th-grade NAEP reading and mathematics assessments and two content areas from the ACT WorkKeys assessment: Reading for Information and Applied Mathematics.</p><p>The <a href='https://www.nagb.gov/focus-areas/reports/preparedness-research/docs/content-alignment/landing/WorkKeys-NAEP_Math_Content_Comparison.html'>final report</a> for the mathematics study was submitted in August 2010, and the <a href='https://www.nagb.gov/focus-areas/reports/preparedness-research/docs/content-alignment/landing/WorkKeys-NAEP_Reading_Content_Comparison.html'>final report</a> for the reading study was submitted in October 2010 (ACT, Inc., 2010a; ACT, Inc. 2010b).</p><p>In general, the findings indicated that the knowledge and skills assessed by NAEP are much broader than those assessed by WorkKeys in both reading and mathematics. As the title suggests, WorkKeys assesses reading for information, while this is only one of the reading skills assessed by NAEP. For mathematics, the area of overlap was largely related to the NAEP items assessing number operations, which is a very small component of the grade 12 mathematics NAEP.</p>", "tier2": "<p>The alignment studies with WorkKeys used the design document developed by Norman Webb for the content alignment studies in the Governing Board's 12th-grade NAEP preparedness research.</p><p><strong>Key Conclusions of the Reading Alignment Study for the WorkKeys Reading for Information Assessment and 12th-Grade NAEP</strong></p><ul><li>The 12th-grade NAEP reading assessment covers a broader range of reading skills than the WorkKeys Reading for Information assessment does. Most of the WorkKeys items that were aligned with NAEP objectives were related to locating/recalling information and causal relations. WorkKeys items that aligned to the \"integrate/interpret\" tasks in NAEP targeted objectives that require the examinee to connect ideas, draw conclusions and provide supporting information, and determine word meaning in context. No WorkKeys items included in this study require the examinee to critique or evaluate the reading passage.</li><li>Unlike NAEP, the WorkKeys assessment focuses on workplace communications, especially policies and instructions, and their application to workplace situations. WorkKeys objectives that are not assessed by the NAEP items include applying complex, multistep, conditional instructions to similar and new workplace situations; determining the meaning of work-related acronyms, jargon, and technical terms; and figuring out and applying general principles contained in informational documents to similar and new workplace situations.</li><li><em>Depth of Knowledge (DOK) on the two assessments.</em> The average DOK level for both NAEP content standards and assessment items was higher than the DOK level for WorkKeys (National Assessment Governing Board, 2010).</li></ul><p><strong>Key Conclusions of the Mathematics Alignment Study for the WorkKeys Applied Mathematics Assessment and 12th-grade NAEP</strong></p><ul><li>The 12th-grade NAEP mathematics assessment covers a broader range of mathematics skills than the WorkKeys Applied Mathematics assessment does, particularly in geometry, data analysis, statistics, probability, and algebra. Most of the WorkKeys items aligned with NAEP objectives related to number operations and measurement.</li><li>The WorkKeys assessment focuses on the application of foundational mathematics skills in workplace situations. Numerous objectives in the WorkKeys Applied Mathematics assessment, , such as performing conversions, determining the best deal, finding errors, and calculating discounts or markups, are not measured by 12th-grade NAEP.</li><li>Depth of Knowledge (DOK) on the two assessments. The average DOK level for NAEP was higher than the DOK level for WorkKeys (National Assessment Governing Board, 2010).</li></ul>", "footnoteIndexes": [3, 4, 19]},
{"code": "march2010-november2010", "date": "March 2010–November 2010", "title": "Content alignment studies conducted for NAEP and the SAT", "category": "Efforts to Evaluate Feasibility of Reporting Preparedness", "tier1": "<p>WestEd conducted content alignment studies for the SAT and 12th-grade NAEP mathematics and reading assessments in March 2010 and presented the final reports, <a href='https://www.nagb.gov/assets/documents/what-we-do/preparedness-research/content-alignment/SAT-NAEP_Reading_Content_Comparison.pdf'>Comprehensive Report: Alignment of 2009 NAEP Grade 12 Reading and SAT Critical Reading</a> and <a href='https://www.nagb.gov/content/nagb/assets/documents/what-we-do/preparedness-research/content-alignment/SAT-NAEP_Math_Content_Comparison.pdf'>Comprehensive Report: Alignment of 2009 NAEP Grade 12 Mathematics and SAT Mathematics</a>, to the Governing Board on November 24, 2010 (WestEd, 2010a; WestEd, 2010b).</p><p>Key findings included the following:</p><ul><li>In reading, the greatest commonality between NAEP and SAT is in their shared emphasis on the broad skills of integrating and interpreting informational and literary texts.</li><li>In mathematics, both NAEP and the SAT assess almost the same content areas, with similar emphasis for each.</li></ul>", "tier2": "<p><strong>Additional Conclusions of the Reading Alignment Study for the SAT and 12th-Grade NAEP</strong></p><ul><li>The results of the item alignments show a broader range of cognitive complexity in the NAEP items.</li><li>Approximately two-thirds of the NAEP items and over 90 percent of the SAT items aligned to the \"integrate/interpret\" NAEP standard.</li><li>All of the NAEP items and about three-quarters of the SAT items were aligned to the passage-based reading-comprehension portion of the SAT. The 12th-grade NAEP reading assessment does not align with the sentence-completion portion of the SAT assessment.</li><li>Both tests emphasize many of the same or closely related specific skills, including inferring/analyzing the main idea and author's purpose, the tone and attitude of an author or character, the use of rhetorical strategies, and connections among ideas, perspectives, or problems.</li><li>Depth of Knowledge (DOK) on the two assessments. The average DOK level for both NAEP content standards and NAEP items was lower than for SAT. No SAT items were coded to DOK level 1 (National Assessment Governing Board, 2010).</li></ul><p><strong>Additional Conclusions of the Mathematics Alignment Study for the SAT and 12th- Grade NAEP</strong></p><ul><li>Regarding alignment to the NAEP framework, both SAT items (around 38 percent) and NAEP items (around 34 percent) had the highest percentages of their overall alignments to the Algebra standard. The smallest percentage of total NAEP items was found to align to Data Analysis, Statistics, and Probability (around 11 percent), while the lowest percentage of SAT items was aligned to the Measurement standard (around 8 percent).</li><li>The percentages of alignments to each SAT standard were relatively evenly distributed in both assessments and similar in distribution across assessments. The SAT Algebra standard had the highest number of items from each assessment aligned—around 36 percent of the SAT items and 32 percent of the NAEP items. For both assessments, the next highest rate of alignment was for the Geometry standard, with around 34 percent of the SAT items and 27 percent of the NAEP items aligned to that standard. The largest difference in emphasis was found for alignment to the SAT Data, Statistics, and Probability content standard—around 21 percent of the NAEP items and only 9 percent of the SAT items were aligned to this standard.</li><li>Depth of Knowledge (DOK) on the two assessments. The two assessments are similar in the average DOK levels of items. While most items in both assessments were found to be DOK Level 2, NAEP items had a wider range of DOK, with more items coded to Level 1 and Level 3. The Level 3 items often involved application of concepts through short or extended constructed-response items. The SAT included only multiple-choice items. NAEP had more items coded at DOK Level 3 than SAT had (National Assessment Governing Board, 2010).</li></ul>", "footnoteIndexes": [46, 47, 19]},
{"code": "april2010-november2010", "date": "April 2010–November 2010", "title": "Content alignment studies conducted for NAEP and ACCUPLACER", "category": "Efforts to Evaluate Feasibility of Reporting Preparedness", "tier1": "<p>WestEd conducted the content alignment studies for ACCUPLACER and 12th-grade NAEP in mathematics and reading in April 2010. They presented the final reports, <a href='https://www.nagb.gov/focus-areas/reports/preparedness-research/docs/content-alignment/landing/ACCUPLACER-NAEP_Reading_Content_Comparison.html'>Comprehensive Report: Alignment of 2009 NAEP Grade 12 Reading and ACCUPLACER Reading Comprehension</a> and <a href='https://www.nagb.gov/focus-areas/reports/preparedness-research/docs/content-alignment/landing/ACCUPLACER-NAEP_Math_Content_Comparison.html'>Comprehensive Report: Alignment of 2009 NAEP Grade 12 Mathematics and ACCUPLACER Mathematics Core Tests</a>, to the Governing Board on November 24, 2010 (WestEd, 2010c; WestEd, 2010d).</p><p>The reading alignment study revealed that the greatest commonality between the two tests is in their shared emphasis on the broad skills of comprehending and interpreting informational text, primarily through inferential reasoning.</p><p>The mathematics alignment study revealed that the greatest area of commonality between the assessments is in the areas of number properties and operations, followed by algebra.</p>", "tier2": "<p><strong>Key Conclusions of the Reading Alignment Study for ACCUPLACER and 12th-Grade NAEP</strong></p><ul><li>A majority of both ACCUPLACER and NAEP items were aligned to the NAEP \"integrate/interpret\" objective.</li><li>Items from each assessment had the highest rates of alignment to the ACCUPLACER \"inferences\" objective, and a higher percentage of NAEP items than ACCUPLACER items were aligned to this objective.</li><li>NAEP items aligned only to the ACCUPLACER reading-comprehension objectives; NAEP items did not align to sentence relationships.</li><li>NAEP addresses literary and informational text, while ACCUPLACER does not address reading skills specific to literary text.</li><li>Up to 15 of the items in the NAEP item pool were unaligned to ACCUPLACER.</li><li>ACCUPLACER items did not align to the NAEP \"critique/evaluate\" reading objective.</li><li>Depth of Knowledge (DOK) on the two assessments. For alignment to the NAEP framework, the NAEP items were found to meet DOK consistency in all standards. The ACCUPLACER items met DOK consistency only for items in the NAEP \"locate/recall\" standard. The majority of ACCUPLACER items aligned to the \"interpret/integrate\" standard had a lower DOK level than that of the standard.</li><li>For alignment to the ACCUPLACER specifications, the ACCUPLACER items were found to meet DOK consistency in all objectives. The NAEP items met DOK consistency in the objectives to which there were alignments but not for \"sentence relationships\" to which no NAEP items were aligned (National Assessment Governing Board, 2010).</li></ul><p><strong>Key Conclusions of the Mathematics Alignment Study for ACCUPLACER and 12th-Grade NAEP</strong></p><ul><li>Most (90 percent) of the ACCUPLACER items were aligned to NAEP objectives in either number properties and operations or algebra, and the items aligned to a limited number of objectives within those content areas.</li><li>NAEP items were aligned to 75 of the 87 ACCUPLACER objectives and were distributed quite evenly across the three ACCUPLACER content areas.</li><li>The greatest areas of difference are in measurement, geometry, and data analysis. Many NAEP items assessing these content areas could not be aligned to the ACCUPLACER items.</li><li>NAEP items were more evenly and broadly distributed across the NAEP objectives than were the ACCUPLACER items.</li><li>Depth of Knowledge (DOK) on the two assessments. For alignment to the NAEP framework, the NAEP items were found to meet DOK consistency in all standards. For alignment to the ACCUPLACER framework, DOK was analyzed as a range of depth of knowledge. NAEP items aligned to the ACCUPLACER framework were coded at DOK Levels 1–3, with roughly two-thirds of the NAEP items at DOK Level 2. Almost all ACCUPLACER items aligned to DOK Level 1 or Level 2. The ACCUPLACER items aligned to the arithmetic content area were about evenly split between DOK Levels 1 and 2; nearly three-quarters of the items aligned to the elementary algebra content were coded as Level 1; and a little over two-thirds of the ACCUPLACER items aligned to college mathematics were coded at Level 2 (National Assessment Governing Board, 2010).</li></ul>", "footnoteIndexes": [48, 49, 19]},
{"code": "july2010", "date": "July 2010", "title": "Design developed for Judgmental Standard Setting studies", "category": "Efforts to Produce Reference Points for Reporting Preparedness", "tier1": "<p>ACT, Inc., developed a design document for use in the Judgmental Standard Setting (JSS) studies for the 12th-grade NAEP preparedness research. ACT provided a draft of the design document, and the Governing Board staff finalized it.<p>ACT, Inc., also submitted a list of 20 occupations to include in the standard-setting studies for job training programs. The Governing Board selected five occupations for standard setting: automotive master mechanic; computer support specialist; licensed practical nurse; heating, ventilation, and air-conditioning technician; and pharmacy technician.</p>", "tier2": "<p>The Technical Panel on 12th-Grade Preparedness Research (Technical Panel) had recommended the development of the document to help ensure that standardization is maintained across the various JSS studies. The goal of the JSS studies was to set a score on the 12th-grade NAEP assessment of mathematics and reading to represent the performance required to meet the Governing Board's definition of preparedness for both higher education and for job training in specific occupations.</p><p>ACT, Inc., used criteria recommended by the Technical Panel to identify potential occupations to include in the studies of job training programs.</p><p>The five occupations selected from the list meeting the technical criteria were chosen because they are familiar to the public, they include a variety of occupational categories including trades and medical areas, and job training programs exist for them in most vocational-technical college programs.</p>"},
{"code": "october2010", "date": "October 2010", "title": "Pilot benchmarking study conducted with Texas colleges and universities", "category": "Efforts to Produce Reference Points for Reporting Preparedness", "tier1": "<p>A benchmarking study was designed for administering the 12th-grade NAEP assessment to college freshmen entering Texas colleges and universities. The average score of their performance was to be used as a reference point on the 12th-grade NAEP reporting scale for comparison to other data in the preparedness research program. Pilot study data were collected in September and October 2010; however, low response rates led to the decision to cancel further plans for the study.</p>", "tier2": "<p>The Texas Commissioner of Higher Education had agreed to have higher education institutions participate in the study, and nine institutions accepted his invitation as volunteers to participate in a pilot study. A feasibility study conducted as interviews with administrators in these institutions in April 2010 indicated that there was a high level of interest and participation in the study. A small-scale pilot study was launched in September, and data collection ended on October 15, 2010. From an eligible sample of 1,234 students, only 255 participated, prompting the decision to cancel further plans for the study.</p>"},
{"code": "november2010-december2011", "date": "November 2010–December 2011", "title": "Statistical linking study results for NAEP and the SAT presented to the Governing Board", "category": "Efforts to Produce Reference Points for Reporting Preparedness", "tier1": "<p>At the November 2010 Governing Board meeting, ETS researchers presented to the Committee on Standards, Design and Methodology (COSDAM). They shared results from statistical linking studies for 12th-grade NAEP with the SAT in reading and mathematics and provided preliminary information regarding the linking data from Florida (National Assessment Governing Board, 2010).</p><p>The final analysis of data for 2009 12th-grade NAEP performance in mathematics and reading in relation to the SAT data for these subject-specific tests was reported to the Governing Board in December 2011. The statistical relationship between NAEP and SAT for mathematics was considerably stronger than for reading—strong enough to warrant a concordance between scores for NAEP and SAT in mathematics. However, projection analyses were again used for both mathematics and reading so that the methodology and interpretations for results would be the same for both subjects (National Assessment Governing Board, 2011).</p>", "tier2": "<p>ETS researchers provided updates on the statistical linking studies for 12th-grade NAEP with the SAT in reading and mathematics to COSDAM at most quarterly meetings of the Governing Board. The researchers also attended several meetings of the technical advisors for 12th-grade NAEP preparedness to provide updates and to collect recommendations from the advisors.</p><p>The preliminary results that ETS provided for the November 2010 meeting included the following:</p><ul><li>The sample match rate for the SAT and NAEP data was approximately 33 percent, which was considered a good rate, given the SAT participation rate of 36 percent of public school students. The correlation between NAEP and SAT data for reading was lower than for mathematics. In fact, the correlation for reading was too low for concordance analyses, so the decision was to report both reading and mathematics linking results using projection analyses instead. The linking data were to produce reference points on the NAEP score scale to evaluate relative to results from other preparedness studies (judgmental standard setting and data from the higher education survey, for example). At their meeting in June 2010, the technical advisors recommended the use of a conditioning model and elimination of outliers for linking the data, and the correlations improved as a result. The correlations were 0.73 for NAEP reading and SAT verbal and 0.90 for NAEP mathematics and SAT mathematics.</li><li>The match for Florida students in the 12th-grade state NAEP sample and the Florida students taking the ACT was approximately 47 percent. Approximately 65 percent of Florida students in the 2010 class were reported as taking the ACT. The correlations for NAEP data and ACT data for the sample of Florida students taking both assessments was 0.77 for mathematics and 0.59 for reading. The sample of Florida students taking the WorkKeys assessment was small, and the match rate with NAEP was only around 6 percent. This eliminated it as a source of information for projecting a reference point on the NAEP scale to represent preparedness for workplace training (National Assessment Governing Board, 2010).</li></ul><p>The final results reported in December 2011 included the following:</p><ul><li>The statistical relationship between NAEP and SAT for mathematics was considerably stronger than for reading—strong enough to warrant a concordance between scores for NAEP and SAT in mathematics. However, projection analyses were again used for both mathematics and reading so that the methodology and interpretations for results would be the same for both subjects. Scores at the <em>NAEP Proficient</em> cut score of NAEP mathematics were associated with an SAT score for students having approximately an 80 percent probability of a B- or higher in a first-year college course, whereas scores at the <em>NAEP Proficient</em> cut score of NAEP reading were associated with an SAT score for students having only a 50 percent probability of at least a B-. Thus, if a .50 probability were used for reporting college preparedness for both subjects, the mathematics preparedness score would be lower than the <em>NAEP Proficient</em> score, and the reading preparedness score would be at the <em>NAEP Proficient</em> score. If a .80 probability were used, the preparedness score for mathematics would be at the <em>NAEP Proficient</em> score, and the reading preparedness score would be well above the <em>NAEP Proficient</em> score. NAEP reporting could show preparedness scores associated with several probabilities (such as 50, .67, and .80) as preparedness reference points on the NAEP scale (National Assessment Governing Board, 2011).</li></ul>", "footnoteIndexes": [19, 20]},
{"code": "april2011", "date": "April 2011", "title": "Pilot study conducted for Judgmental Standard Setting studies", "category": "Efforts to Produce Reference Points for Reporting Preparedness", "tier1": "<p>The Judgmental Standard Setting (JSS) pilot study was conducted to evaluate all aspects of the procedures, logistics, room arrangements, and so forth planned for the JSS studies. The goal was to identify modifications that might be needed for the operational sessions (WestEd, 2011a).</p><p>Based on findings from the pilot study, changes to the study design were made to help panelists with the tasks of developing borderline definitions and for describing the knowledge, skills, and abilities required to perform well on items. The results showed low replicability between the pairs of panelists for each postsecondary area, and the results for the two postsecondary areas were judged to be lacking in face validity.</p>", "tier2": "<p>The pilot study was conducted according to the procedures in the design document planned for use in the operational studies. The pilot study included replicate panels in both mathematics and reading for higher education and for job training for an occupation as an automotive master technician. Panelists' responses to evaluation questionnaires indicated that they understood the process and were confident in it, but they were not pleased with the results. The patterns of the pilot study results were unlike those of most successful standard settings in several respects. There was low evidence of replicability of results based on the cut scores of the two replicate panels for each subject and postsecondary area. Job training panelists struggled with the fact that NAEP assessed academic skills rather than skills more relevant to the workplace. Some of the tasks were challenging for them as a result. They also complained about the lack of relevance observed for many items (WestEd, 2011a).</p>", "footnoteIndexes": [50]},
{"code": "may2011-february2012", "date": "May 2011–February 2012", "title": "Operational Judgmental Standard Setting studies conducted for college and five job training programs", "category": "Efforts to Produce Reference Points for Reporting Preparedness", "tier1": "<p>There was again relatively low reliability based on the results for each pair of replicate panels.</p><p>Some consideration was given to the possibility of setting one cut score in mathematics for college preparedness and one for job training, given the standard error bands for the cut sores. For reading, the standard errors were such as to suggest a single cut score for preparedness for both the job training areas and college course work.</p><p>Based on the findings and the advice of staff, the Governing Board decided against reporting the results of these studies as preparedness indicator scores on the NAEP scale.</p><p>The first operational Judgmental Standard Setting (JSS) study was conducted in May 2011. It included two replicate panels: onefor  reading and mathematics, and one for job training for an occupation as an automotive master technician, as well as replicate panels for college preparedness in both reading and mathematics.</p><p>The second operational study was conducted in June 2011 and included panels for training programs for occupations as licensed practical nurses and pharmacy technicians.</p><p>The final operational study was conducted in June and July 2011 and included panels for training programs for occupations in heating, ventilation, and air conditioning and as computer support technicians.</p><p>Two reports under the title of \"National Assessment of Educational Progress Grade 12 Preparedness Research Project Judgmental Standard Setting (JSS) Studies\" were produced for the Governing Board: a <a href='https://www.nagb.gov/focus-areas/reports/preparedness-research/docs/judgmental-standard-setting/landing/Standard_Setting_Process.html'>Process Report</a> delivered November 28, 2011, and a <a href='https://www.nagb.gov/focus-areas/reports/12th-grade-achievement-in-america.html'>Technical Report</a> delivered February 22, 2012 (WestEd, 2011a; WestEd, 2011b).</p>", "tier2": "<p>Despite changes made to procedures based on the pilot study, panelists still struggled with the tasks. They again found many items to be irrelevant to the needs of their training programs.</p><p><strong>Key Findings</strong></p><p>In mathematics, the cut scores set by seven of the 12 separate panels were within the <em>NAEP Proficient</em> score range, and five were within the <em>NAEP Basic</em> score range. Only one of the job training occupational groups plus the college group had cut scores for both replicate panels within the same (<em>NAEP Proficient</em>) score range. For reading, seven of the 12 panels set cut scores within the <em>NAEP Basic</em> score range, and five were within the <em>NAEP Proficient</em> score range.</p><p>There was relatively low reliability between the two replicate panels for each postsecondary area. For mathematics, three of the six replicate pairs had a standard error between 6 and 10 on a 300-point scale. For reading, five of the six pairs had a standard error of 7−11 on a 500-point score scale. A special study revealed that the variability between replicate panel cut score results was not attributable to facilitator effects.</p><p>The mathematics knowledge and skills assessed by the 12th-grade NAEP assessments were generally much more complex and rigorous than those required for the job training programs. The prior mathematics training of the instructors did not prepare them to make judgments about all of the mathematics skills included in the 12th-grade NAEP assessment. The 12th-grade NAEP reading assessment included many literary passages and required knowledge and skills of students that were judged as not necessary for entry into job training programs (WestEd, 2011a).</p>", "footnoteIndexes": [50, 51]},
{"code": "march2012", "date": "March 2012", "title": "Comprehensive review of results from 12th-grade NAEP 2009 preparedness research studies, including reports on final results of statistical linking studies at the national level as well as at the state level for Florida students", "category": "Efforts to Produce Reference Points for Reporting Preparedness", "tier1": "<p>A <a href='https://www.nagb.gov/content/nagb/assets/documents/what-we-do/quarterly-board-meeting-materials/2013-12/COSDAM-report-dec-2013.pdf'>report</a> on the preparedness research studies and findings to date were presented to the full Governing Board during the March 2012 Governing Board meeting (National Assessment Governing Board, 2012a).</p><p>ETS researchers presented a comprehensive <a href='https://www.nagb.gov/content/nagb/assets/documents/what-we-do/quarterly-board-meeting-materials/2012-03/Attachment%20A-1A%20Report%20on%20Florida%20Linking.pdf'>report</a> of the final results from statistical linking studies for 12th-grade NAEP with the SAT in reading and mathematics and with SAT, ACT, and ACCUPLACER performance of Florida students (Moran et al., 2012, p. 56).</p><p>Results from two of five job training areas were presented from a study to compare course curriculum materials to the NAEP frameworks and the preparedness performance-level descriptors developed for the Judgmental Standard Setting (JSS) studies for job training programs.</p><p>In addition, preliminary results from a national survey of postsecondary institutions to collect data on tests and test scores used for placement of entering students into remedial/nonremedial courses were shared at this meeting.</p><p>An update on the development of the validity framework document was also shared with the Committee on Standards, Design and Methodology (COSDAM).</p><p>Based on the lack of confirmatory evidence for the preparedness cut scores resulting from the JSS studies, the anecdotal evidence gathered during the JSS studies, follow-up analyses of data, and preliminary results of curriculum studies underway for courses in the job training programs, the staff of the Governing Board recommended that the results of the JSS studies not be reported as reference points for preparedness. The recommendation was accepted and approved by COSDAM.</p>", "tier2": "<p><strong>Statistical Linking Studies:</strong> Results for linking studies using Florida data were found to be mutually confirming and reasonable and to provide validity evidence in support of the reference points produced for the national-level SAT linking studies with NAEP. The final scores from the linking studies were presented as reference points on the NAEP scale for analysis, relative to findings from other studies, including the statistical analyses of linking studies with Florida data.</p><p>Other variables were included in the analyses of Florida student data: self-reported program of high school study, public college (community college/state university system) attended after high school, first-year college coursetaking (remedial courses/nonremedial courses), and first-year college grade point average (below B- versus at or above B-) (Moran et al., 2012, p. 56).</p><p><strong>Studies of Job Training Programs:</strong> Staff shared data from two job training programs in the automotive master technician occupational area for which the first set of analysis had been completed for a study by the Educational Policy Improvement Center (EPIC) in order to compare course curriculum materials to the NAEP frameworks and the preparedness performance-level descriptors developed for the JSS studies for job training programs.</p><p>Based on comparisons of course components for those two programs, about three-fourths of the knowledge, skills, and abilities included in the NAEP objectives for the 12th-grade mathematics assessment were absent from the course materials. For reading, however, slightly over half of the 12th-grade framework objectives were represented by knowledge, skills, and objectives included in the beginning courses for these two programs (National Assessment Governing Board, 2012a).</p><p>Survey of postsecondary tests and test scores: An overview of the study was provided and plans for future reporting discussed. Final results from the survey of tests and test scores used by higher education institutions for placement of entering students into remedial/nonremedial courses were reported in November 2012 (National Assessment Governing Board, 2012a).</p><p>Validity framework: The first version of a validity framework had been shared with COSDAM for discussion at the August 2010 meeting. Staff announced a plan to share an updated validity framework with statements of findings and evidence in support of those findings with the Technical Advisory Panel for Preparedness Research in April 2012 for their review and evaluation. Plans for vetting and finalizing the Validity Framework were announced (National Assessment Governing Board, 2012a).</p>", "footnoteIndexes": [21, 14]},
{"code": "may2012", "date": "May 2012", "title": "Recommendations discussed by the Governing Board for reporting preparedness for the 2009 NAEP", "category": "Governing Board Actions and Decisions", "tier1": "<p>The Committee on Standards, Design and Methodology (COSDAM) met in a joint session with the Reporting and Dissemination Committee for the briefing on and discussion of preparedness research and plans for reporting. They stressed the importance of having meaningful results to report.</p><p>The full Governing Board was then briefed about the preliminary determinations that NAEP could be used to inform the national discussion about the academic preparedness of U.S. students and more than one academic preparedness reference point on the NAEP scales could be reported. Although there were no data to report for workplace preparedness, there was support for moving forward with plans to reporting preparedness research findings for the 2009 NAEP using only data for college preparedness (National Assessment Governing Board, 2012b).</p>", "tier2": "<p>Concerns were expressed both in the committee session and in the full Governing Board session regarding the use of different probabilities and the <em>NAEP Proficient</em> cut point to represent preparedness. Staff had discussed reporting options and developed new recommendations to present for reporting. The recommendation from staff was to report two reference points: (1) likely to be successful in freshman year in college and (2) likely to need remediation. These reference points were largely based on the statistical linking studies for NAEP with the SAT. The staff recommendation was to use the <em>NAEP Proficient</em> cut score for each subject as the reference point for preparedness for \"freshman year success.\" The <em>NAEP Proficient</em> cut score for mathematics 12th-grade NAEP is associated with an 80 percent probability of scoring 500 on the mathematics SAT assessment, and the College Board has established 500 as the benchmark score having a .67 probability of earning a B- freshman year grade point average. For reading, for which there is a lower correlation between the NAEP and SAT, the probability of scoring 500 on the critical reading SAT assessment is .5 at the <em>NAEP Proficient</em> cut score. Staff recommended using different probabilities of freshman year success (80 percent and 50 percent) for both the subjects and reporting preparedness relative to the <em>NAEP Proficient</em> achievement level for both probabilities. Using the same probability for both would have resulted in either a relatively low percentage of predicted freshman year success for students based on NAEP reading scores or a relatively high percentage of predicted freshman year success for students in mathematics.</p><p>COSDAM recommended that the policy report and the technical report for the preparedness research results be released at the same time.</p><p>The full Governing Board met in session to be briefed on the preparedness research results and plans for reporting. Two questions were presented to the Governing Board:</p><ol><li>Can NAEP be used to inform the national discussion about the academic preparedness of U.S. students?</li><li>Will more than one academic preparedness reference point on the NAEP scales be reported?</li></ol><p>The Governing Board's executive director, Cornelia Orr, stated that the answer to both questions was affirmative but not without some caveats. She then provided a review of the information that had previously been shared with the two committees (above) for consideration by the full Governing Board membership. Two reference points were recommended for each subject for grade 12 preparedness: likely to be successful in college and likely to need remediation. Although there were no data to report for workplace preparedness, some Governing Board members voiced support for moving forward with plans to reporting preparedness research findings for the 2009 NAEP using only data for college preparedness (National Assessment Governing Board, 2012b).</p>", "footnoteIndexes": [22]},
{"code": "august2012", "date": "August 2012", "title": "Recommendations approved for limited reporting of preparedness results and release of research", "category": "Governing Board Actions and Decisions", "tier1": "<p>Based on all of the research conducted to report on the preparedness of 12th-graders, the following recommendations were made by the Reporting and Dissemination Committee:</p><ul><li>\"Any policy or general public report on 12th grade preparedness—and attendant release activities and promotion—should be delayed until further research is conducted based on the 2013 NAEP.</li><li>Completed research studies should be released as a package on the Internet, accompanied by brief summaries of their methodology and key findings without a public event by the Board or its 12th Grade Preparedness Commission. The release should be accompanied by a statement on the status of the Board's preparedness research and future plans\" (National Assessment Governing Board, 2012c, p. 33).</li></ul>", "tier2": "<p>The Governing Board voted to release all completed studies together in the fall of 2012, along with brief summaries of methodology and findings. However, no report aimed at policymakers or the general public would be released at this time. Additional research would be conducted with data from the 2013 NAEP assessments.</p><p>The Governing Board staff would also meet with the National Center for Education Statistics (NCES) to plan cooperation in implementing this future research (National Assessment Governing Board, 2012c).</p>", "footnoteIndexes": [23]},
{"code": "november2012-december2012", "date": "November 2012−December 2012", "title": "Higher education survey results reported for course placement tests and cut scores and plans for Phase II Preparedness Research for 2013 NAEP", "category": "Efforts to Produce Reference Points for Reporting Preparedness", "tier1": "<p>A draft of the report, <a href='https://files.eric.ed.gov/fulltext/ED539918.pdf'>Tests and Cut Scores Used for Student Placement in Postsecondary Education: Fall 2011</a>, presented the findings from a national survey of higher education institutions that was conducted to produce data on placement scores used for qualifying for college credit courses and scores that would require remediation (Fields & Parsad, 2012). The final report was released in January 2013.</p><p>Plans for Phase II of the academic preparedness research for the 2013 NAEP 12th-grade assessment were discussed by the Governing Board at the December 2012 meeting (National Assessment Governing Board, 2012d).</p>", "tier2": "<p>A nationally representative sample of higher education institutions was included in the higher education study. Separate survey forms were used for two-year and four-year institutions. A total of 1,560 institutions were included in the sample, and 1,340 responded. The placement scores were to be used as reference points on the NAEP mathematics and reading scales for evaluation relative to other preparedness research reference points: the scores produced by the linking studies for the 12th-grade NAEP assessments of mathematics and reading with the SAT and with the Florida assessment database that includes placement test and other assessment data. The Committee on Standards, Design and Methodology (COSDAM) suggested additional analyses to be added for the final report.</p><p>COSDAM also discussed the online technical report and noted a need to clarify the connections among the various research studies.</p><p>Finally, COSDAM began discussing plans for preparedness research studies with the 2013 NAEP grade 12 assessment. Plans included adding two states in the 2013 12th-grade state NAEP, for a total of 13 states; replicating the linking studies for NAEP and SAT; undertaking a linking study for NAEP and the ACT; and possibly conducting a statistical linking study for 8th-grade NAEP with the EXPLORE® assessment by ACT. The goal was to include more longitudinal studies for reporting 2013 12th-grade NAEP preparedness.</p><p>The importance of the finding that NAEP is not appropriate for studying preparedness for entry into job training programs was noted, and COSDAM cautioned staff that there was likely to be no benefit to the Governing Board from any additional research about preparedness for job training programs. COSDAM unanimously agreed that no new research should be undertaken in this area (National Assessment Governing Board, 2012d).</p>", "footnoteIndexes": [9, 24]},
{"code": "december2012-february2013", "date": "December 2012–February 2013", "title": "Technical report finalized and released for 2009 12th-grade NAEP preparedness research", "category": "Governing Board Actions and Decisions", "tier1": "<p>The draft version of the online technical report for the 2009 12th-grade NAEP preparedness research was presented to the Governing Board by the executive director and the deputy executive director in December 2012.</p><p>The <a href='https://www.nagb.gov/focus-areas/reports/preparedness-research.html'>online technical report</a> for Phase I of the Governing Board's academic preparedness research was released to the public on February 15, 2013 (National Assessment Governing Board, 2013a).</p>", "footnoteIndexes": [25]},
{"code": "january2013-march2013", "date": "January 2013–March 2013", "title": "NAEP assessments conducted, including representative samples for 13 states to receive 12th-grade results", "category": "Efforts to Produce Reference Points for Reporting Preparedness", "tier1": "<p>Grade 12 NAEP assessments in mathematics and reading were administered to a nationally representative sample as well as to state samples in 13 states. A total of 1,970 high schools were in the assessment, with 45,900 12th-graders assessed in reading and 46,500 assessed in mathematics.</p>", "tier2": "<p></p>"},
{"code": "march2013", "date": "March 2013", "title": "Curriculum study of job training program courses presented to the Governing Board", "category": "Efforts to Evaluate Feasibility of Reporting Preparedness", "tier1": "<p>The Educational Policy Improvement Center (EPIC) presented the results of the <a href='https://www.nagb.gov/content/nagb/assets/documents/what-we-do/preparedness-research/judgmental-standard-setting-studies/job-training-programs-curriculum-study.pdf'>National Assessment of Educational Progress Grade 12 Preparedness Research Project Job Training Programs Curriculum Study</a> to the Governing Board. Few prerequisites (knowledge, skills, and abilities at program entry) in either subject were found across the job training program materials examined, which meant that only a small number of the objectives assessed by NAEP were in the course materials for any of these training programs (Educational Policy Improvement Center, 2013).<p><p>The study also compared requirements for entry-level training in the five occupational areas to the performance requirements for preparedness identified in the Judgmental Standard Setting (JSS) studies for those occupations. The findings were not comparable; the curriculum requirements were less rigorous.</p><p>This study was conducted as supplementary research to the formal research plan.</p>", "tier2": "<p>The design of this study promised to provide more concrete information regarding the knowledge, skills, and abilities (KSAs) assessed in 12th-grade NAEP relative to those required for entry to job training programs. This study was designed to identify and evaluate the following sets of KSAs:</p><ol><li>Prerequisite KSAs for entry-level courses for training programs based on course materials</li><li>KSAs taught in entry-level courses for job training programs, based on course materials</li><li>KSAs expected for courses at the conclusion of job training programs</li><li>KSAs identified as prerequisites through analysis of job training course materials compared to KSAs identified as part of the Judgmental Standard Setting (JSS)</li></ol><p>A total of 122 institutions provided packets of materials for 85 mathematics courses and 80 courses with a heavy reading demand. The materials were collected from institutions offering courses in the same five training programs identified by the Governing Board for the 12th-grade NAEP preparedness research program and included in the JSS studies conducted in 2011 (Educational Policy Improvement Center, 2013).</p>", "footnoteIndexes": [6]},
{"code": "july2013", "date": "July 2013", "title": "The Nation's Report Card and 12th-Grade Academic Preparedness: A National Symposium convened in Washington, DC", "category": "Commissions and Technical Panels", "tier1": "<p><em>The National Commission on NAEP 12th-Grade Assessment and Reporting</em> hosted a symposium in Washington, DC, to discuss the results of the Governing Board's research on 12th-grade academic preparedness for college and job training and the feasibility of The Nation's Report Card's serving as an indicator of preparedness.</p>", "tier2": "<p>This symposium was the final of seven regional symposia held throughout the country over the two-year tenure of the commission. Former governor of Mississippi Ronnie Musgrave chaired the seven-member commission.</p><p>For the final symposium in Washington, DC, there were two panels. The panel for college preparedness included Mitchell Chester, Chester Finn, Glenda Glover, and Carmel Martin, and the panel for job training preparedness included Anthony Carnevale, Roberts Jones, Jacqueline King, Carl Mack, and Cheryl Oldham.</p>"},
{"code": "august2013", "date": "August 2013", "title": "Preparedness indicators and statements adopted by the Governing Board to report results from 2013 NAEP", "category": "Governing Board Actions and Decisions", "tier1": "<p>At the August 2013 meeting of the Governing Board, the Committee on Standards, Design and Methodology (COSDAM) engaged in a lengthy discussion of the proposed <a href='https://www.nagb.gov/content/dam/nagb/en/documents/what-we-do/quarterly-board-meeting-materials/2013-08/tab06-cosdam-08-2013.pdf'>validity argument</a> and plans for reporting. After discussions from previous meetings were reviewed, a plan was proposed and adopted to report the preparedness indicator scores on 2013 NAEP for both reading and mathematics associated with using a probability of scoring at or above 500 on the SAT in the subject; this is the score that the College Board had established to predict with a .67 probability that a student would earn at least a B- in the first year of college. That score in reading corresponded to the <em>NAEP Proficient</em> cut score; for mathematics, it fell within the <em>NAEP Basic</em> achievement-level range.</p><p>The following <a href='https://www.nagb.gov/content/nagb/assets/documents/what-we-do/quarterly-board-meeting-materials/2013-12/tab02-august-2013-board-meeting-minutes.pdf'>text</a> is the motion recommended by COSDAM and approved by the Governing Board:</p><ul><li>\"Given the design, content, and characteristics of the NAEP 12th-grade reading assessment, and the strength of relationships between NAEP scores and NAEP content to other relevant measures of college academic preparedness: the percentage of students scoring at or above a score of 302 on Grade 12 NAEP scale in reading is a plausible estimate of the percentage of students who possess the knowledge, skills, and abilities in reading that would make them academically prepared for college.</li><li>A score of 302 corresponds to the cut-score for the Proficient achievement level in 12th grade reading\" (National Assessment Governing Board, 2013b, p. 53).</li></ul><p>The motion included the same wording for mathematics with 163 as the score to represent preparedness: \"A score of 163 in mathematics is between the cut-scores for the Basic and Proficient achievement levels in 12th grade mathematics\" (National Assessment Governing Board, 2013b, p. 54).</p><p>The percentage of 12th-graders scoring at or above each of these preparedness reference points was <a href='https://www.nationsreportcard.gov/reading_math_g12_2013/#/preparedness'>reported</a> for the 2013 NAEP assessments in each subject: 38 percent in reading and 39 percent in mathematics.</p>", "tier2": "<p>The August 2013 vote was based on discussions that began at the May 2012 quarterly meeting of the Governing Board when COSDAM reviewed and discussed a draft of a validity argument for the 12th-grade preparedness research that focused on the <em>NAEP Proficient</em> achievement level as the preparedness reference point for each subject. COSDAM revealed that they were unsure about the emphasis on the NAEP Proficient achievement level as the reference point for preparedness in their discussions at the March 2013 meeting, and they continued to have reservations at the May 2013 meeting.</p><p>A presentation with statements of findings regarding preparedness of 12th-graders had been provided to both the Executive Committee and the full Governing Board during the May 2013 meeting. A prototype chapter for reporting preparedness research had been developed and shared with COSDAM for an interim discussion in July, and it was discussed further at the August 2013 meeting. The validity argument presented at the August 2013 meeting used the average NAEP score for 12th-grade students scoring at the SAT college readiness benchmark for estimating preparedness for both reading and mathematics. This resulted in a preparedness score for reading at the <em>NAEP Proficient</em> achievement level and for mathematics lower than the <em>NAEP Proficient</em> achievement level. Despite some lingering concerns, COSDAM concluded that they were generally comfortable with the overall direction of the validity argument and the plans for reporting in terms of the validity argument. They adopted the recommendation to report preparedness for reading and mathematics using the same probability of success in the first year of college and recommended it for approval by the Governing Board (National Assessment Governing Board, 2013c).</p>", "footnoteIndexes": [26, 27]},
{"code": "december2013", "date": "December 2013", "title": "Plans initiated for preparedness research for the 2013 NAEP assessments", "category": "Efforts to Produce Reference Points for Reporting Preparedness", "tier1": "<p>At the December 2013 Governing Board meeting, staff <a href='https://www.nagb.gov/content/nagb/assets/documents/what-we-do/quarterly-board-meeting-materials/2013-12/COSDAM-report-dec-2013.pdf'>reported</a> that data-sharing agreements were being developed with five states that participated in state-based assessments for the 2013 NAEP assessments (National Assessment Governing Board, 2013d).</p>", "tier2": "<p>The data-sharing agreements were needed to support linking studies for reporting 12th-grade preparedness on the 2013 NAEP assessments in reading and mathematics.</p><p>Agreements with ACT, Inc., were underway for linking studies with the ACT assessment and 12th-grade NAEP data for the 2013 assessment and for a linking study between ACT EXPLORE® and 8th-grade NAEP in mathematics and reading to examine the feasibility of inferences about student performance on NAEP that indicate being \"on track\" for college preparedness.</p><p>Statistical linking agreements for use of SAT data from the College Board were no longer possible (National Assessment Governing Board, 2013d).</p>", "footnoteIndexes": [28]},
{"code": "may2014", "date": "May 2014", "title": "Report released summarizing preparedness research for NAEP", "category": "Governing Board Actions and Decisions", "tier1": "<p>A review of the preparedness research studies with the 2009 12th-grade NAEP in mathematics and reading was reported by Ray Fields for the National Assessment Governing Board in <a href='https://www.nagb.gov/content/nagb/assets/documents/what-we-do/preparedness-research/NAGB-indicator-of-preparedness-report.pdf'>Towards The National Assessment of Educational Progress (NAEP) as an Indicator of Academic Preparedness for College and Job Training</a>. This report was included as a reference in The Nation's Report Card reporting results for 12th-grade NAEP preparedness research (Fields, R., 2014).</p>", "tier2": "<p>In conclusion, Fields noted \"the research results support inferences about NAEP performance and academic preparedness for college at the national level. The research results do not support inferences about NAEP performance and academic preparedness for job training\" (2014, p. 4).</p>", "footnoteIndexes": [8]},
{"code": "june2014", "date": "June 2014", "title": "Curriculum study of college courses presented to the Governing Board", "category": "Efforts to Evaluate Feasibility of Reporting Preparedness", "tier1": "<p>The Educational Policy Improvement Center (EPIC) presented its report, <a href='https://files.eric.ed.gov/fulltext/ED571807.pdf'>National Assessment of Educational Progress Grade 12 Preparedness Research College Course Content Analysis Study: Final Report</a>, to the Governing Board on June 12, 2014. The study analyzed course materials to determine how the knowledge, skills, and abilities (KSAs) required as prerequisites for an entry-level course compare to the NAEP frameworks and items assessed for the mathematics and reading 12th-grade NAEP subjects. The goal was to determine how the KSAs compare to the borderline preparedness descriptions developed in the Judgmental Standard Setting (JSS) studies for the Governing Board's preparedness research and to identify how course information can be used in future NAEP preparedness research.</p><p>\"The results from the study indicate[d] that most of the prerequisite KSAs for both mathematics courses and courses that require extensive college-level reading are reflected in the NAEP frameworks\" (Educational Policy Improvement Center, 2014, p. 10). In reading, the prerequisite KSAs that were identified in the study described a depth of understanding and level of cognitive demand beyond the minimum preparedness requirements described in the JSS studies. In mathematics, the prerequisite KSAs were similar or less cognitively complex than the minimum preparedness requirements in the JSS studies.</p>", "tier2": "<p>A sample of 151 two- and four-year institutions contributed 160 sets of entry-level course materials, with half for mathematics courses and half for courses with a heavy reading requirement (Educational Policy Improvement Center, 2014).</p>", "footnoteIndexes": [7]},
{"code": "november2014", "date": "November 2014", "title": "The Content Alignment between the NAEP and WorkKeys Assessments: Final Report released", "category": "Efforts to Evaluate Feasibility of Reporting Preparedness", "tier1": "<p>This <a href='https://www.nagb.gov/focus-areas/reports/preparedness-research/docs/content-alignment/landing/naep_workkeys_final.html'>study</a> was conducted by Human Resources Research Organization (HumRRO) to evaluate the content alignment of both the 12th-grade NAEP and 8th-grade NAEP assessments in reading and mathematics with the ACT WorkKeys assessments to determine the feasibility of using the NAEP assessments to measure academic preparedness for entry in job training programs (Human Resources Research Organization, 2014).</p><p>The results of this study indicated that neither the 12th-grade NAEP nor the 8th-grade NAEP assessment content was appropriate for making inferences regarding academic preparedness for entry into job training programs.</p>", "tier2": "<p>The content alignments for 12th-grade revealed that NAEP items did not represent the WorkKeys content areas: 52 percent of the WorkKeys mathematics content targets and 75 percent of the WorkKeys reading content targets were not matched to NAEP items. Similarly, NAEP content domains were not matched by WorkKeys items.</p><p>Although there was more overlap in WorkKeys content targets by 8th-grade NAEP items, the results of this study did not support using the 8th-grade NAEP assessments to measure academic preparedness for job training (Human Resources Research Organization, 2014).</p>", "footnoteIndexes": [10]},
{"code": "january2015-march2015", "date": "January 2015–March 2015", "title": "NAEP assessments conducted", "category": "Efforts to Produce Reference Points for Reporting Preparedness", "tier1": "<p>A nationally representative sample of 12th-graders participated in NAEP in 2015. Budget constraints forced cancellation of plans to include state assessments of 12th-grade NAEP.</p>"},
{"code": "february2015", "date": "February 2015", "title": "Comprehensive report released about the evaluations of NAEP 12th-grade reading and mathematics frameworks and item pools as measures of academic preparedness for college and job training", "category": "Efforts to Evaluate Feasibility of Reporting Preparedness", "tier1": "<p><a href='https://www.nagb.gov/focus-areas/reports/preparedness-research/docs/content-alignment/landing/naep-framework-evaluation-comprehensive-report.html'>Evaluation of NAEP 12th Grade Reading and Mathematics Frameworks and Item Pools as Measures of Academic Preparedness for College and Job Training: Comprehensive Report</a> summarized the results of several studies conducted for the Governing Board to evaluate the utility of further research into using NAEP as a measure of preparedness for entry into job training programs.</p><p>The conclusions indicated that NAEP is not an appropriate measure of preparedness for job training programs and that valid inferences cannot be made about preparedness for job training programs based on NAEP performance (Human Resources Research Organization, 2015).</p>", "tier2": "<p>The research included:</p><ul><li>an alignment study of 12th-grade NAEP frameworks and item pools as measures of academic preparedness for entry into job training programs;</li><li>an examination of the frameworks and item pools for these subjects for the 8th-grade NAEP to determine whether the 8th-grade assessment would be a better match to job training requirements than the 12th-grade NAEP assessment would be;</li><li>a content alignment between NAEP and ACT's WorkKeys assessments in reading and mathematics; and</li><li>comparisons of NAEP knowledge, skills, and abilities requirements to the U.S. Department of Labor's occupational information network, or O*NET, for five target occupations (Human Resources Research Organization, 2015).</li></ul>", "footnoteIndexes": [11]},
{"code": "august2015", "date": "August 2015", "title": "Updates on Preparedness Research with Grade 8 NAEP for Reading and Mathematics", "category": "Efforts to Produce Reference Points for Reporting Preparedness", "tier1": "<p>The Committee on Standards, Design and Analysis (COSDAM) reviewed a report on content alignment between the 2013 NAEP assessments in reading and in mathematics and the ACT EXPLORE®. ETS researchers presented analyses of statistical linking studies for NAEP and EXPLORE® performances for three states: Kentucky, Tennessee, and North Carolina (National Assessment Governing Board, 2015).</p>", "tier2": "<p>The content alignment study <a href='https://www.nagb.gov/content/nagb/assets/documents/what-we-do/preparedness-research/8th-grade/content-alignment/naep-explore-math-report.pdf'>reported</a> by NORC at the University of Chicago indicated considerable overlap in the content covered by grade 8 NAEP and EXPLORE® in mathematics, but the comparison of reading assessments revealed differences between the two with respect to emphasis and complexity. The final report on this project was released November 2015 (NORC at the University of Chicago, 2015).</p><p>The statistical relationship studies pointed to a relationship between NAEP grade 8 assessments and ACT EXPLORE® such that the \"on track\" benchmark projected near the NAEP Proficient achievement level for grade 8 in both subjects. The results indicated that 30 percent to 35 percent of the students in the three states included in the study would be \"on track\" for academic preparedness for college.</p><p>COSDAM members requested a future agenda topic to allow a discussion of further longitudinal studies with state partners and whether to continue to seek additional state partners.</p>", "footnoteIndexes": [29, 37]},
{"code": "march2016", "date": "March 2016", "title": "Reports released for preparedness research on the 2013 8th-grade NAEP", "category": "Efforts to Produce Reference Points for Reporting Preparedness", "tier1": "<p>Statistical relationship studies were conducted to link the 2013 8th-grade NAEP results for students in <a href='https://www.nagb.gov/content/nagb/assets/documents/what-we-do/preparedness-research/8th-grade/statistical-relationship/ky-preparedness-grade-8.pdf'>Kentucky</a>, <a href='https://www.nagb.gov/content/nagb/assets/documents/what-we-do/preparedness-research/8th-grade/statistical-relationship/nc-preparedness-grade-8.pdf'>North Carolina</a>, and <a href='https://www.nagb.gov/content/nagb/assets/documents/what-we-do/preparedness-research/8th-grade/statistical-relationship/tn-preparedness-grade-8.pdf'>Tennessee</a>, with their scores on ACT EXPLORE® in reading and mathematics. Final analyses were reported (Sgammato et al., 2016a; Sgammato et al., 2016b; Sgammato et al., 2016c).</p>", "tier2": "<p>Content alignment studies were conducted prior to the statistical linking studies to determine whether the content overlap between the two assessments for each subject were sufficiently comparable to warrant the statistical linking studies (NORC, at the University of Chicago, 2015). The statistical relationship analyses were conducted on behalf of the Governing Board by ETS researchers to determine the scores on the 8th-grade NAEP assessment associated with the ACT EXPLORE® College Readiness Benchmarks.</p>", "footnoteIndexes": [41, 42, 43]},
{"code": "august2016", "date": "August 2016", "title": "Statistical linking studies conducted for 12th-grade NAEP and the ACT in Tennessee and Michigan and NAEP and the SAT in Massachusetts", "category": "Efforts to Produce Reference Points for Reporting Preparedness", "tier1": "<p>The report to the Committee on Standards, Design and Methodology (COSDAM) included a <a href='https://www.nagb.gov/content/nagb/assets/documents/what-we-do/quarterly-board-meeting-materials/2016-08/04-committee-on-standards-design-and-methodology.pdf'>report</a> on initial findings for statistical linking studies conducted by NAEP program researchers at ETS for 12th-grade NAEP and for both reading and mathematics in three states. The statistical relationship was found to be moderate but not sufficient for concordance of scores. As was the case for the national statistical linking of grade 12 NAEP with the SAT, the linking studies for these states required the use of projection analyses and produced projected college preparedness scores for NAEP relative to the readiness benchmark score of 500 for the SAT and 22 for the ACT. In each state, the projected NAEP score was at or very near the <em>NAEP Proficient</em> achievement level for reading and below the <em>NAEP Proficient</em> level for mathematics (National Assessment Governing Board, 2016a).</p>", "tier2": "<p>The analysis of data for Massachusetts was for grade 12 NAEP with SAT scores for 2013. About 2,400 Massachusetts 12th-grade students were in the matched sample for the study in each subject. The match rate was 74 percent for reading and 76 percent for mathematics. The correlation between NAEP and SAT scores for reading was .74 and for mathematics was .87—both too low for concordance; therefore, statistical projection was used for the linking analyses. The NAEP reading scale score where students would have a reasonable probability of academic preparedness is 302 for reading and 164 for mathematics, based on the SAT college readiness benchmark scores. These scores are at the <em>NAEP Proficient</em> cut score for reading and 12 points below the <em>NAEP Proficient</em> cut score for mathematics. For Massachusetts 12th-graders, the <em>NAEP Proficient</em> cut score projects to the SAT reading scale at 490, which is below the SAT college readiness benchmark of 500. For mathematics, the NAEP grade 12 <em>NAEP Proficient</em> cut score projects to 540, which is above the readiness benchmark.</p><p>The study for Michigan included approximately 2,900 12th-graders for reading and 3,100 for mathematics. The ACT assessment is used as the state assessment for high school in Michigan, and the match rate for ACT and NAEP scores for each subject was 95 percent. The correlation between the ACT and NAEP was .73 for reading and .83 for mathematics—too low for concordance; therefore, statistical projection was used for the linking analyses. The NAEP scale score that represents the ACT college readiness benchmark for reading is 308, which is higher than the grade 12 <em>NAEP Proficient</em> cut score of 301 for reading. The NAEP cut score representing the ACT college readiness benchmark for mathematics is 169, which is lower than the grade 12 <em>NAEP Proficient</em> cut score of 176 for mathematics. The grade 12 <em>NAEP Proficient</em> cut score projects to the ACT readiness benchmark score of 22 for reading, and the mathematics <em>NAEP Proficient</em> cut score projects to the ACT scale at 24, which is above the ACT readiness benchmark.</p><p>The study for Tennessee included approximately 3,000 12th-grade students for reading and 3,200 for mathematics. Tennessee uses the ACT assessment as the state assessment for high school, and the match rate was 89 percent for reading and 90 percent for mathematics. The correlation between scores for the NAEP and ACT assessments for reading was .73 and for mathematics was .83—too low for concordance; therefore, statistical projection was used for the linking study. The NAEP scale score representing the SAT college readiness benchmark for reading is 301, which is just one point below the grade 12 <em>NAEP Proficient</em> cut score for reading. The NAEP scale score representing the SAT college readiness benchmark for mathematics is 168, which is 8 points below the <em>NAEP Proficient</em> cut score. The NAEP grade 12 <em>NAEP Proficient</em> cut score for reading projects to a score of 23 on the ACT reading scale, which is 1 point higher than the ACT college readiness benchmark.  The NAEP grade 12 <em>NAEP Proficient</em> cut score for mathematics projects to a score of 25 on the ACT mathematics scale, which is 3 points higher than the ACT college readiness benchmark.</p><p>COSDAM members noted the similarity in results for the ACT and SAT college preparedness indicators and expressed a desire for more information based on longitudinal data.</p><p>COSDAM also engaged in review and discussion of efforts to measure preparedness for job training via grade 12 assessments. They reviewed the earlier decision to end this endeavor and discussed the possibility of examining other NAEP subjects in relation to preparedness for job training and to explore other assessments for linking (National Assessment Governing Board, 2016a).</p>", "footnoteIndexes": [30]},
{"code": "november2016", "date": "November 2016", "title": "Approval of Governing Board's Strategic Vision, including developing approaches to measure preparedness", "category": "Governing Board Actions and Decisions", "tier1": "<p>The <a href='https://www.nagb.gov/content/nagb/assets/documents/newsroom/press-releases/2016/nagb-strategic-vision.pdf'>Strategic Vision</a> called for expanding the outreach efforts and relationships aimed at raising awareness about The Nation's Report Card and resources associated with the NAEP data collection. The Strategic Vision specified development of \"new approaches to measure complex skills required for transition to postsecondary education and career\" (National Assessment Governing Board, 2016b). The Strategic Vision was to guide work through 2020.</p>", "footnoteIndexes": [31]},
{"code": "august2017", "date": "August 2017", "title": "Ad Hoc Committee on Measures of Postsecondary Preparedness established", "category": "Governing Board Actions and Decisions", "tier1": "<p>The Ad Hoc Committee was appointed in August 2017 and charged with actions to achieve Strategic Vision #10 regarding the development of new approaches to measuring student transition to postsecondary education and careers.</p><p>The Ad Hoc Committee was advised to consider options for achieving this goal that might include changes to existing legislation. The Ad Hoc Committee of the Governing Board was to report recommendations no later than the November 2018 quarterly Governing Board meeting (National Assessment Governing Board, 2017).</p>", "footnoteIndexes": [32]},
{"code": "november2018", "date": "November 2018", "title": "Ad Hoc Committee report calling for the development of a postsecondary preparedness dashboard delivered", "category": "Commissions and Technical Panels", "tier1": "<p>The Ad Hoc Committee's <a href='https://www.nagb.gov/content/nagb/assets/documents/publications/reports-papers/preparedness/final-recommendations-report-20181117.pdf'>final report</a> was delivered in November 2018 and focused on the use of existing measures, whenever possible, to provide more information about the skills and abilities needed for postsecondary preparedness (National Assessment Governing Board, 2018).</p>", "tier2": "<p>In its final report, the Ad Hoc Committee called for the development of a postsecondary preparedness dashboard through a collaboration of the Governing Board and the National Center for Education Statistics (NCES). This dashboard was to provide a comprehensive set of indicators to display the \"academic knowledge, literacies, cross-cutting cognitive skills, and intra- and inter-personal skills that are essential abilities for all students graduating high school to be prepared for postsecondary endeavors.\" A list of tasks for the Governing Board centered around the creation of a conceptual framework to describe \"universal skills that represent postsecondary preparedness.\" A set of tasks was also given to NCES, including the development of a prototype dashboard and a design for the development of new NAEP postsecondary preparedness indicators (National Assessment Governing Board, 2018).</p>", "footnoteIndexes": [33]},
{"code": "january-march2019", "date": "January–March 2019", "title": "NAEP assessments conducted", "category": "Efforts to Produce Reference Points for Reporting Preparedness", "tier1": "<p>NAEP assessments in mathematics and reading were administered to a nationally representative sample only; no state assessments were included. This was the first digitally based assessment administered at grade 12 in each of these subjects. For reading, the assessment included 26,700 12th-graders from 1,780 schools. For mathematics, the assessment included 25,400 12th-graders from 1,770 schools.</p>"},
{"code": "march2019", "date": "March 2019", "title": "Statistical linking study results for NAEP and the ACT presented to the Governing Board", "category": "Efforts to Produce Reference Points for Reporting Preparedness", "tier1": "<p>The <a href='https://www.nagb.gov/content/dam/nagb/en/documents/what-we-do/preparedness-research/statistical-relationships/Preparedness-Research-NAEP-ACT-linking_508.pdf'>final report</a> of the statistical linking studies for NAEP and the national data for the ACT assessment in both reading and mathematics were presented to the Committee on Standards, Design and Methodology COSDAM).</p>", "tier2": "<p>The overall match rate for NAEP and ACT data was 41 percent for reading and 42 percent for mathematics. The correlation between performance scores on NAEP and ACT for mathematics was .87 and for reading was .75. These correlations were judged to be too low for concordance analyses.</p><p>As was done for the NAEP-SAT linking studies, projection analyses were conducted for these linking studies for NAEP and ACT. The cut scores for the <em>NAEP Proficient</em> level in each NAEP subject (302 for reading and 176 for mathematics) were projected to the ACT scale, and the ACT readiness benchmark scores (22 for each subject) were projected to the NAEP scale. The ACT readiness benchmarks project to the NAEP scale in a pattern similar to the projection of SAT readiness benchmarks: near the <em>NAEP Proficient</em> cut score for reading and below the <em>NAEP Proficient</em> cut score for mathematics. The ACT reading readiness benchmark projected to 301 on the NAEP reading scale, and the ACT mathematics readiness benchmark projected to 167 on the NAEP mathematics scale. The <em>NAEP Proficient</em> cut score for reading projected to the ACT readiness score (22) for reading, and the <em>NAEP Proficient</em> cut score for mathematics projected one point higher (23) on the ACT scale for mathematics.</p>"},
{"code": "november2019", "date": "November 2019", "title": "Postsecondary preparedness conceptual framework and prototype dashboard presented to the Governing Board", "category": "Governing Board Actions and Decisions", "tier1": "<p>At its November Governing Board meeting, the Governing Board discussed the conceptual framework of a dashboard for providing a comprehensive set of indicators of knowledge, skills, and abilities needed in order to be prepared for postsecondary endeavors—a <a href='https://www.nagb.gov/content/nagb/assets/documents/what-we-do/quarterly-board-meeting-materials/2020-03/02-november-2019-meeting-minutes.pdf'>recommendation</a> of the Ad Hoc Committee on Measures of Postsecondary Preparedness. In general, there was little support for moving forward with the dashboard (National Assessment Governing Board, 2019).</p>", "tier2": "<p>The conceptual framework was shared in advance to prepare for the discussion of the prototype dashboard. The conceptual framework was intended to be the driver of content for the dashboard. Discussion groups met in breakout sessions and reported back to the Governing Board. There was a perceived need for more contextual understanding of how the postsecondary preparedness research had originated and developed to date (National Assessment Governing Board, 2019).</p>", "footnoteIndexes": [34]},
{"code": "september2020", "date": "September 2020", "title": "Governing Board adopts Strategic Vision 2025", "category": "Governing Board Actions and Decisions", "tier1": "<p>The National Assessment Governing Board adopted <a href='https://www.nagb.gov/governing-board/strategic-vision.html'>Strategic Vision 2025</a> at the virtual meeting held September 29, 2020. The first Strategic Vision was adopted in November 2016 to guide the work through 2020. For their work in the updated Strategic Vision, the Board added \"engage\" to the previous major goals, \"inform\" and \"innovate.\" \"Engage\" was included specifically to \"help stakeholders understand how the Governing Board and NAEP can illuminate important skills for post-secondary pathways.\" This was a renewal of commitment to continue research for reporting preparedness on the grade 12 NAEP scale for reading and for mathematics (National Assessment Governing Board, 2020).</p>", "tier2": "<p>The overall match rate for NAEP and ACT data was 41 percent for reading and 42 percent for mathematics. The correlation between performance scores on NAEP and ACT for mathematics was .87 and for reading was .75. These correlations were judged to be too low for concordance analyses. As was done for the NAEP-SAT linking studies, projection analyses were conducted for these linking studies for NAEP and ACT. The cut scores for the <em>NAEP Proficient</em> level in each NAEP subject (302 for reading and 176 for mathematics) were projected to the ACT scale, and the ACT readiness benchmark scores (22 for each subject) were projected to the NAEP scale. The ACT readiness benchmarks project to the NAEP scale in a pattern similar to the projection of SAT readiness benchmarks: near the <em>NAEP Proficient</em> cut score for reading and below the <em>NAEP Proficient</em> cut score for mathematics. The ACT reading readiness benchmark projected to 301 on the NAEP reading scale, and the ACT mathematics readiness benchmark projected to 167 on the NAEP mathematics scale. The <em>NAEP Proficient</em> cut score for reading projected to the ACT readiness score (22) for reading, and the <em>NAEP Proficient</em> cut score for mathematics projected one point higher (23) on the ACT scale for mathematics.</p>", "footnoteIndexes": [35]},
{"code": "december2020", "date": "December 2020", "title": "Linking study results for longitudinal Michigan data presented to the Governing Board", "category": "Efforts to Produce Reference Points for Reporting Preparedness", "tier1": "<p>At the December 2020 meeting of the Committee on Standards, Design and Methodology (COSDAM), a <a href='https://www.nagb.gov/content/dam/nagb/en/documents/what-we-do/preparedness-research/statistical-relationships/Longitudinal-statistical-relationship-for-Michigan-NAEP-examinees-FINAL-January-2021-508.pdf'>linking study</a> was presented by NAEP program researchers at ETS for review and discussion by COSDAM. This study was an analysis of data from the Michigan longitudinal database covering a six-year period from 2013 to 2019. The longitudinal Michigan data were linked with grade 12 NAEP Michigan data from the 2013 assessment. (See discussion of the linking study for 2013 grade 12 NAEP Michigan data and Michigan ACT data in August 2016.)</p><p>The analysis of longitudinal data focused on projections to the NAEP score scale representing postsecondary remedial coursetaking, college grade point averages (GPAs), and college attendance/degree completion. These data points are compared to data from other studies that produced points on the NAEP scale in order to examine the extent to which they provide evidence  of the reliability of results and support for the validity of using grade 12 NAEP data for reporting preparedness.</p><p>These data were seen to provide strong confirmatory and validity evidence for using the NAEP preparedness benchmarks to indicate 12th-graders' academic preparedness for college (Xi et al., 2020).</p>", "tier2": "<p><u>Remedial coursetaking</u>: The average NAEP scale score for students who enrolled in college but were never enrolled in a postsecondary remedial course was 298 for reading and 164 for mathematics. For students who were enrolled in one or more postsecondary remedial courses, the average NAEP scale score was 274 for reading and 136 for mathematics.</p><p><u>First-year GPA</u>: Students who enrolled in college and had at least a B- (2.5 or higher) for their first year GPA had an average grade 12 NAEP score of 305 for reading and 169 for mathematics. Students for whom GPA data were lower than 2.5 had an average grade 12 NAEP score of 281 for reading and 153 for mathematics.</p><p><u>Consecutive years in college after high school</u>: Data were reported for students who never enrolled in college; those who enrolled in the first year after high school but not the second year; those who enrolled in the first two years but not the third; and so forth through those enrolled in years 1–5 but not the sixth year after high school; and those enrolled all six years after high school. The average NAEP score for each subject increased with increasing consecutive years in college after high school up through the fourth year, which had the highest average grade 12 NAEP scores. NAEP scores for students attending years 1–5 but not 6 and years 1–6 were lower than those who presumably graduated in four years, and those attending all six years were second highest in each subject. The average NAEP scores ranged from a low of 269 in reading and 137 in mathematics for students who did not enroll in college the year after high school to a high of 311 in reading and 172 in mathematics for students who were consecutively enrolled for the four years after high school but not the fifth.</p><p><u>Average grade 12 NAEP score for students who graduated within six years after high school</u>: Students who were never enrolled in college during the six years following high school had an average NAEP score of 266 for reading and 133 for mathematics as 12th-graders. Students who enrolled in college but never earned a degree or certificate had an average NAEP score of 280 for reading and 148 for mathematics as 12th-graders. Students who enrolled in college and achieved a degree or certificate that was less than a BA had an average NAEP score of 290 for reading and 156 for mathematics as 12th-graders. Students who received a BA or MA within six years of high school had an average NAEP score of 311 for reading and 176 for mathematics (Xi et al., 2020).</p>", "footnoteIndexes": [54]}
], "footnotes": ["<p><em>Achieve, Inc. (2005). Recommendations to the National Assessment Governing Board on aligning 12th grade NAEP with college and workplace expectations: Reading</em>. Prepared for the National Assessment Governing Board. <a href = \"https://www.nagb.gov/content/nagb/assets/documents/commission/researchandresources/Achieve%20Reading%20Report.pdf\">https://www.nagb.gov/content/nagb/assets/documents/commission/researchandresources/Achieve%20Reading%20Report.pdf</a></p>",
"<p>Achieve, Inc. (2006). <em>Recommendations to the National Assessment Governing Board on aligning the 12th grade NAEP with college, workplace, and military expectations: Mathematics</em>. Prepared for the National Assessment Governing Board. <a href = \"https://www.nagb.gov/content/nagb/assets/documents/commission/researchandresources/Achieve-Mathematics-Report.pdf\">https://www.nagb.gov/content/nagb/assets/documents/commission/researchandresources/Achieve-Mathematics-Report.pdf</a></p>",
"<p>ACT, Inc. (2009). <em>NAEP in relation to the ACT College Readiness Standards and Benchmarks in Reading and Mathematics</em>. Prepared for the National Assessment Governing Board. <a href = \"https://www.nagb.gov/focus-areas/reports/preparedness-research/docs/content-alignment/landing/ACT-NAEP_Math_and_Reading_Content_Comparison.html\">https://www.nagb.gov/focus-areas/reports/preparedness-research/docs/content-alignment/landing/ACT-NAEP_Math_and_Reading_Content_Comparison.html</a></p>",
"<p>ACT, Inc. (2010a). <em>The alignment of the NAEP Grade 12 Mathematics assessment and the WorkKeys Applied Mathematics assessment</em>. Prepared for the National Assessment Governing Board. <a href = \"https://www.nagb.gov/focus-areas/reports/preparedness-research/docs/content-alignment/landing/WorkKeys-NAEP_Math_Content_Comparison.html\">https://www.nagb.gov/focus-areas/reports/preparedness-research/docs/content-alignment/landing/WorkKeys-NAEP_Math_Content_Comparison.html</a></p>",
"<p>ACT, Inc. (2010b). <em>The alignment of the NAEP Grade 12 Reading assessment and the WorkKeys Reading for Information assessment</em>. Prepared for the National Assessment Governing Board. <a href = \"https://www.nagb.gov/focus-areas/reports/preparedness-research/docs/content-alignment/landing/WorkKeys-NAEP_Reading_Content_Comparison.html\">https://www.nagb.gov/focus-areas/reports/preparedness-research/docs/content-alignment/landing/WorkKeys-NAEP_Reading_Content_Comparison.html</a></p>",
"<p>Alley, W. (2005). <em>Twelfth grade NAEP and readiness for entrance into the military: Validity issues and methodological options</em>. Prepared for the National Assessment Governing Board. <a href = \"https://www.nagb.gov/content/nagb/assets/documents/what-we-do/Alley.doc\">https://www.nagb.gov/content/nagb/assets/documents/what-we-do/Alley.doc</a></p>",
"<p>Educational Policy Improvement Center. (2013). <em>National Assessment of Educational Progress grade 12 preparedness research project job training programs curriculum study</em>. Prepared for the National Assessment Governing Board. <a href = \"https://www.nagb.gov/content/nagb/assets/documents/what-we-do/preparedness-research/judgmental-standard-setting-studies/job-training-programs-curriculum-study.pdf\">https://www.nagb.gov/content/nagb/assets/documents/what-we-do/preparedness-research/judgmental-standard-setting-studies/job-training-programs-curriculum-study.pdf</a></p>",
"<p>Educational Policy Improvement Center. (2014). <em>National Assessment of Educational Progress grade 12 preparedness research college course content analysis study: Final report. Prepared for the National Assessment Governing Board. <a href = \"https://files.eric.ed.gov/fulltext/ED571807.pdf\">https://files.eric.ed.gov/fulltext/ED571807.pdf</a></p>",
"<p>Fields, R. (2014). <em>Towards the National Assessment of Educational Progress (NAEP) as an indicator of academic preparedness for college and job training</em>. National Assessment Governing Board. <a href = \"https://files.eric.ed.gov/fulltext/ED582999.pdf\">https://files.eric.ed.gov/fulltext/ED582999.pdf</a></p>",
"<p>Fields, R., & Parsad, B. (2012). <em>Tests and cut scores used for student placement in postsecondary education: Fall 2011</em>. National Assessment Governing Board. <a href = \"https://files.eric.ed.gov/fulltext/ED539918.pdf\">https://files.eric.ed.gov/fulltext/ED539918.pdf</a></p>",
"<p>Human Resources Research Organization. (2014). <em>The content alignment between the NAEP and WorkKeys assessments: Final report</em>. Prepared for the National Assessment Governing Board. <a href = \"https://www.nagb.gov/content/nagb/assets/documents/what-we-do/preparedness-research/content-alignment/naep_workkeys_final.pdf\">https://www.nagb.gov/content/nagb/assets/documents/what-we-do/preparedness-research/content-alignment/naep_workkeys_final.pdf</a></p>",
"<p>Human Resources Research Organization. (2015). <em>Evaluation of NAEP 12th grade reading and mathematics frameworks and item pools as measures of academic preparedness for college and job training: Comprehensive report</em>. Prepared for the National Assessment Governing Board. <a href = \"https://www.nagb.gov/focus-areas/reports/preparedness-research/docs/content-alignment/landing/naep-framework-evaluation-comprehensive-report.html\">https://www.nagb.gov/focus-areas/reports/preparedness-research/docs/content-alignment/landing/naep-framework-evaluation-comprehensive-report.html</a><p>",
"<p>Kirst, M. (2003). <em>College preparation and grade 12 NAEP</em>. Prepared for the National Assessment Governing Board. <a href = \"https://www.nagb.gov/content/nagb/assets/documents/naep/College-Preparation-and-Grade-12-NAEP.doc\">https://www.nagb.gov/content/nagb/assets/documents/naep/College-Preparation-and-Grade-12-NAEP.doc</a></p>",
"<p>Laird, B. (2004). <em>Standard-setting on admissions tests in higher education: The uses of admissions and placement tests by selected groups of two- and four-year colleges and universities</em>. Prepared for the National Assessment Governing Board. <a href = \"https://www.nagb.gov/content/nagb/assets/documents/what-we-do/laird.doc\">https://www.nagb.gov/content/nagb/assets/documents/what-we-do/laird.doc</a></p>",
"<p>Moran, R., Freund, D., & Oranje, A. (2012). <em>NAEP 12th Grade preparedness research: Overview of analyses relating Florida students’ performance on NAEP to preparedness indicators and postsecondary performance</em>. [Paper presentation]. The National Assessment Governing Board meeting, Washington, DC, United States. <a href = \"https://www.nagb.gov/content/nagb/assets/documents/what-we-do/quarterly-board-meeting-materials/2012-03/Attachment%20A-1A%20Report%20on%20Florida%20Linking.pdf\">https://www.nagb.gov/content/nagb/assets/documents/what-we-do/quarterly-board-meeting-materials/2012-03/Attachment%20A-1A%20Report%20on%20Florida%20Linking.pdf</a></p>",
"<p>National Assessment Governing Board. (2005). <em>Resolution: Reporting on preparedness of 12th grade students for college-credit course work, training for employment, and entrance into the military</em>. <a href = \"https://www.nagb.gov/content/nagb/assets/documents/policies/resolution-on-preparedness.pdf\">https://www.nagb.gov/content/nagb/assets/documents/policies/resolution-on-preparedness.pdf</a></p>",
"<p>National Assessment Governing Board. (2006). <em>The future of 12th Grade NAEP report of the Ad Hoc Committee on Planning for NAEP 12th Grade Assessments in 2009</em>. <a href = \"https://www.nagb.gov/content/nagb/assets/documents/policies/The%20Future%20of%2012th%20Grade%20NAEP.pdf\">https://www.nagb.gov/content/nagb/assets/documents/policies/The%20Future%20of%2012th%20Grade%20NAEP.pdf</a></p>",
"<p>National Assessment Governing Board. (2008). <em>Committee on Standards, Design and Methodology (COSDAM) briefing materials, August 2008 meeting: Summary report on recommendations of expert content alignment group</em>. Washington, DC.</p>",
"<p>National Assessment Governing Board. (2009). <em>Committee on Standards, Design and Methodology (COSDAM) committee report, March 2009 meeting</em>. Washington, DC.</p>",
"<p>National Assessment Governing Board. (2010). <em>Committee on Standards, Design and Methodology (COSDAM) briefing materials, November 2010 meeting</em>. <a href = \"https://www.nagb.gov/content/nagb/assets/documents/what-we-do/board-committee-reports-and-agendas/2010-11-19-COSDAM.pdf\">https://www.nagb.gov/content/nagb/assets/documents/what-we-do/board-committee-reports-and-agendas/2010-11-19-COSDAM.pdf</a></p>",
"<p>National Assessment Governing Board. (2011). <em>Committee on Standards, Design and Methodology (COSDAM) committee report, December 2011 meeting</em>. <a href = \"https://www.nagb.gov/content/nagb/assets/documents/what-we-do/board-committee-reports-and-agendas/2011-12-02-COSDAM.pdf\">https://www.nagb.gov/content/nagb/assets/documents/what-we-do/board-committee-reports-and-agendas/2011-12-02-COSDAM.pdf</a></p>",
"<p>National Assessment Governing Board. (2012a). <em>Committee on Standards, Design and Methodology (COSDAM) committee report, March 2012 meeting</em>. <a href = \"https://www.nagb.gov/content/nagb/assets/documents/what-we-do/board-committee-reports-and-agendas/2012-03/2012-03-02-COSDAM.pdf\">https://www.nagb.gov/content/nagb/assets/documents/what-we-do/board-committee-reports-and-agendas/2012-03/2012-03-02-COSDAM.pdf</a></p>",
"<p>National Assessment Governing Board. (2012b). <em>Committee on Standards, Design and Methodology (COSDAM) and Reporting and Dissemination committee report, May 2012 meeting</em>. <a href = \"https://www.nagb.gov/content/nagb/assets/documents/what-we-do/board-committee-reports-and-agendas/2012-05/2012-05-18-COSDAM.pdf\">https://www.nagb.gov/content/nagb/assets/documents/what-we-do/board-committee-reports-and-agendas/2012-05/2012-05-18-COSDAM.pdf</a></p>",
"<p>National Assessment Governing Board. (2012c). <em>Official summary of board actions, August 2012 meeting</em>. National Assessment Governing Board. <a href = \"https://www.nagb.gov/content/nagb/assets/documents/what-we-do/quarterly-board-meeting-materials/2012-11/august-2012-meeting-minutes.pdf\">https://www.nagb.gov/content/nagb/assets/documents/what-we-do/quarterly-board-meeting-materials/2012-11/august-2012-meeting-minutes.pdf</a></p>",
"<p>National Assessment Governing Board. (2012d). <em>Official summary of board actions, November −December 2012 meeting. <a href = \"https://www.nagb.gov/content/dam/nagb/en/documents/what-we-do/quarterly-board-meeting-materials/2013-02/Tab%201%20Meeting%20Minutes%20-%20December%202012.pdf\">https://www.nagb.gov/content/dam/nagb/en/documents/what-we-do/quarterly-board-meeting-materials/2013-02/Tab%201%20Meeting%20Minutes%20-%20December%202012.pdf</a></p>",
"<p>National Assessment Governing Board. (2013a). <em>Do our high school seniors have the academic knowledge and skills they need for postsecondary endeavors?</em> <a href = \"https://www.nagb.gov/focus-areas/reports/preparedness-research.html\">https://www.nagb.gov/focus-areas/reports/preparedness-research.html</a></p>",
"<p>National Assessment Governing Board. (2013b). <em>Official summary of board actions, August 2013 meeting</em>. <a href = \"https://www.nagb.gov/content/nagb/assets/documents/what-we-do/quarterly-board-meeting-materials/2013-12/tab02-august-2013-board-meeting-minutes.pdf\">https://www.nagb.gov/content/nagb/assets/documents/what-we-do/quarterly-board-meeting-materials/2013-12/tab02-august-2013-board-meeting-minutes.pdf</a></p>",
"<p>National Assessment Governing Board. (2013c). <em>Committee on Standards, Design, and Methodology (COSDAM) briefing materials, August 2013 meeting</em>. <a href = \"https://www.nagb.gov/content/dam/nagb/en/documents/what-we-do/quarterly-board-meeting-materials/2013-08/tab06-cosdam-08-2013.pdf\">https://www.nagb.gov/content/dam/nagb/en/documents/what-we-do/quarterly-board-meeting-materials/2013-08/tab06-cosdam-08-2013.pdf</a></p>",
"<p>National Assessment Governing Board. (2013d). <em>Committee on Standards, Design and Methodology (COSDAM) and Reporting and Dissemination committee report, December 2013 meeting</em>. <a href = \"https://www.nagb.gov/content/nagb/assets/documents/what-we-do/quarterly-board-meeting-materials/2013-12/COSDAM-report-dec-2013.pdf\">https://www.nagb.gov/content/nagb/assets/documents/what-we-do/quarterly-board-meeting-materials/2013-12/COSDAM-report-dec-2013.pdf</a></p>",
"<p>National Assessment Governing Board. (2015). <em>Committee on Standards, Design and Methodology (COSDAM) committee report, August 2015 meeting</em>. <a href = \"https://www.nagb.gov/content/nagb/assets/documents/what-we-do/board-committee-reports-and-agendas/2015-08/082015-cosdam-committee-report.pdf\">https://www.nagb.gov/content/nagb/assets/documents/what-we-do/board-committee-reports-and-agendas/2015-08/082015-cosdam-committee-report.pdf</a></p>",
"<p>National Assessment Governing Board. (2016a). <em>Committee on Standards, Design and Methodology (COSDAM) briefing materials, August 2016 meeting</em>. <a href = \"https://www.nagb.gov/content/dam/nagb/en/documents/what-we-do/quarterly-board-meeting-materials/2016-08/04-committee-on-standards-design-and-methodology.pdf\">https://www.nagb.gov/content/dam/nagb/en/documents/what-we-do/quarterly-board-meeting-materials/2016-08/04-committee-on-standards-design-and-methodology.pdf</a></p>",
"<p>National Assessment Governing Board. (2016b). <em>National Assessment Governing Board’s strategic vision</em>. <a href = \"https://www.nagb.gov/content/nagb/assets/documents/newsroom/press-releases/2016/nagb-strategic-vision.pdf\">https://www.nagb.gov/content/nagb/assets/documents/newsroom/press-releases/2016/nagb-strategic-vision.pdf</a></p>",
"<p>National Assessment Governing Board. (2017). <em>Official summary of Governing Board actions, November 2017 meeting</em>. <a href = \"https://www.nagb.gov/content/nagb/assets/documents/what-we-do/quarterly-board-meeting-materials/2018-03/02-november-2017-board-meeting-minutes.pdf\">https://www.nagb.gov/content/nagb/assets/documents/what-we-do/quarterly-board-meeting-materials/2018-03/02-november-2017-board-meeting-minutes.pdf</a></p>",
"<p>National Assessment Governing Board. (2018). <em>Final report of the Ad Hoc Committee on Measures of Postsecondary Preparedness</em>. <a href = \"https://www.nagb.gov/content/nagb/assets/documents/publications/reports-papers/preparedness/final-recommendations-report-20181117.pdf\">https://www.nagb.gov/content/nagb/assets/documents/publications/reports-papers/preparedness/final-recommendations-report-20181117.pdf</a></p>",
"<p>National Assessment Governing Board. (2019). <em>Official summary of Governing Board actions, November 2019 meeting</em>. <a href = \"https://www.nagb.gov/content/nagb/assets/documents/what-we-do/quarterly-board-meeting-materials/2020-03/02-november-2019-meeting-minutes.pdf\">https://www.nagb.gov/content/nagb/assets/documents/what-we-do/quarterly-board-meeting-materials/2020-03/02-november-2019-meeting-minutes.pdf</a></p>",
"<p>National Assessment Governing Board. (2020). <em>Strategic Vision 2025 Draft for Discussion and Action</em>. <a href = \"https://www.nagb.gov/content/dam/nagb/en/documents/what-we-do/quarterly-board-meeting-materials/2020-07/10--Proposed-Strategic-Vision-2025-Draft-for-Action.pdf\">https://www.nagb.gov/content/dam/nagb/en/documents/what-we-do/quarterly-board-meeting-materials/2020-07/10--Proposed-Strategic-Vision-2025-Draft-for-Action.pdf</a></p>",
"<p>National Commission on NAEP <em>12th-Grade Assessment and Reporting. (2004). 12th grade student achievement in America: A new vision for NAEP. Prepared for the National Assessment Governing Board</em>. <a href = \"https://www.nagb.gov/focus-areas/reports/12th-grade-achievement-in-america.html#:~:text=12th%20Grade%20Student%20Achievement%20in%20America%3A%20A%20New%20Vision%20for%20NAEP,Early%20in%202003&text=The%20Commission%20was%20instructed%20to,of%20the%20high%20school%20curriculum\">https://www.nagb.gov/focus-areas/reports/12th-grade-achievement-in-america.html#:~:text=12th%20Grade%20Student%20Achievement%20in%20America%3A%20A%20New%20Vision%20for%20NAEP,Early%20in%202003&text=The%20Commission%20was%20instructed%20to,of%20the%20high%20school%20curriculum</a></p>",
"<p>NORC at the University of Chicago. (2015). <em>Alignment between the 2013 NAEP grade 8 mathematics assessment and ACT EXPLORE mathematics assessment</em>. Prepared for the National Assessment Governing Board. <a href = \"https://www.nagb.gov/content/nagb/assets/documents/what-we-do/preparedness-research/8th-grade/content-alignment/naep-explore-math-report.pdf\">https://www.nagb.gov/content/nagb/assets/documents/what-we-do/preparedness-research/8th-grade/content-alignment/naep-explore-math-report.pdf</a></p>",
"<p>Porter, A. (2004). <em>Twelfth grade NAEP as an indicator of college readiness: Validity issues and methodological options</em>. Vanderbilt University. Prepared for the National Assessment Governing Board. <a href = \"https://www.nagb.gov/content/nagb/assets/documents/what-we-do/andy.doc\">https://www.nagb.gov/content/nagb/assets/documents/what-we-do/andy.doc</a></p>",
"<p>Schmitt, N. (2004). <em>Readiness for training standards: Development and validation</em>. Prepared for the National Assessment Governing Board. <a href = \"https://www.nagb.gov/content/nagb/assets/documents/what-we-do/schmitt.doc\">https://www.nagb.gov/content/nagb/assets/documents/what-we-do/schmitt.doc</a></p>",
"<p>Sellman, W. S. (2004). <em>Predicting readiness for military service: How enlistment standards are established</em>. Prepared for the National Assessment Governing Board. <a href = \"https://www.nagb.gov/content/nagb/assets/documents/what-we-do/sellman.doc\">https://www.nagb.gov/content/nagb/assets/documents/what-we-do/sellman.doc</a></p>",
"<p>Sgammato, A., Lin, M., Jerry, L., Freund, D., Michel, R., & Oranje, A. (2016a). <em>NAEP grade 8 academic preparedness research: establishing a statistical relationship between the NAEP and EXPLORE® grade 8 assessments in reading and mathematics for Kentucky students</em>. Educational Testing Service. Prepared for the National Assessment Governing Board. <a href = \"https://www.nagb.gov/content/nagb/assets/documents/what-we-do/preparedness-research/8th-grade/statistical-relationship/ky-preparedness-grade-8.pdf\">https://www.nagb.gov/content/nagb/assets/documents/what-we-do/preparedness-research/8th-grade/statistical-relationship/ky-preparedness-grade-8.pdf</a></p>",
"<p>Sgammato, A., Lin, M., Jerry, L., Freund, D., Michel, R., Xi, N., & Oranje, A. (2016b). <em>NAEP grade 8 academic preparedness research: Establishing a statistical relationship between the NAEP and EXPLORE® grade 8 assessments in reading and mathematics for North Carolina students</em>. Educational Testing Service. Prepared for the National Assessment Governing Board. <a href = \"https://www.nagb.gov/content/nagb/assets/documents/what-we-do/preparedness-research/8th-grade/statistical-relationship/nc-preparedness-grade-8.pdf\">https://www.nagb.gov/content/nagb/assets/documents/what-we-do/preparedness-research/8th-grade/statistical-relationship/nc-preparedness-grade-8.pdf</a></p>",
"<p>Sgammato, A., Lin, M., Jerry, L., Freund, D., Michel, R., Xi, N., & Oranje, A. (2016c). <em>NAEP grade 8 academic preparedness research: Establishing a statistical relationship between the NAEP and EXPLORE® grade 8 assessments in reading and mathematics for Tennessee students</em>. Educational Testing Service. Prepared for the National Assessment Governing Board. <a href = \"https://www.nagb.gov/content/nagb/assets/documents/what-we-do/preparedness-research/8th-grade/statistical-relationship/tn-preparedness-grade-8.pdf\">https://www.nagb.gov/content/nagb/assets/documents/what-we-do/preparedness-research/8th-grade/statistical-relationship/tn-preparedness-grade-8.pdf</a></p>",
"<p>Technical Panel on 12th-Grade Preparedness Research. (2008). <em>Making new links: 12th grade and beyond</em>. Prepared for the National Assessment Governing Board. <a href = \"https://files.eric.ed.gov/fulltext/ED507257.pdf\">https://files.eric.ed.gov/fulltext/ED507257.pdf</a></p>",
"<p>Webb, N. (2009). <em>Design of content alignment studies in mathematics and reading for 12th grade NAEP and other assessments to be used in preparedness research studies</em>. Prepared for the National Assessment Governing Board. <a href = \"https://www.nagb.gov/focus-areas/reports/design-content-alignment-studies.html\">https://www.nagb.gov/focus-areas/reports/design-content-alignment-studies.html</a></p>",
"<p>WestEd. (2010a). <em>Comprehensive report: Alignment of 2009 NAEP grade 12 reading and SAT critical reading</em>. Prepared for the National Assessment Governing Board. <a href = \"https://www.nagb.gov/assets/documents/what-we-do/preparedness-research/content-alignment/SAT-NAEP_Reading_Content_Comparison.pdf\">https://www.nagb.gov/assets/documents/what-we-do/preparedness-research/content-alignment/SAT-NAEP_Reading_Content_Comparison.pdf</a></p>",
"<p>WestEd. (2010b). <em>Comprehensive report: Alignment of 2009 NAEP grade 12 mathematics and SAT mathematics</em>. Prepared for the National Assessment Governing Board. <a href = \"https://www.nagb.gov/content/nagb/assets/documents/what-we-do/preparedness-research/content-alignment/SAT-NAEP_Math_Content_Comparison.pdf\">https://www.nagb.gov/content/nagb/assets/documents/what-we-do/preparedness-research/content-alignment/SAT-NAEP_Math_Content_Comparison.pdf</a></p>",
"<p>WestEd. (2010c). <em>Comprehensive report: Alignment of 2009 NAEP Grade 12 reading and ACCUPLACER reading comprehension</em>. Prepared for the National Assessment Governing Board. <a href = \"https://www.nagb.gov/focus-areas/reports/preparedness-research/docs/content-alignment/landing/ACCUPLACER-NAEP_Reading_Content_Comparison.html\">https://www.nagb.gov/focus-areas/reports/preparedness-research/docs/content-alignment/landing/ACCUPLACER-NAEP_Reading_Content_Comparison.html</a></p>",
"<p>WestEd. (2010d). <em>Comprehensive report: Alignment of 2009 NAEP Grade 12 mathematics and ACCUPLACER mathematics core tests</em>. Prepared for the National Assessment Governing Board. <a href = \"https://www.nagb.gov/focus-areas/reports/preparedness-research/docs/content-alignment/landing/ACCUPLACER-NAEP_Math_Content_Comparison.html\">https://www.nagb.gov/focus-areas/reports/preparedness-research/docs/content-alignment/landing/ACCUPLACER-NAEP_Math_Content_Comparison.html</a></p>",
"<p>WestEd. (2011a). <em>National Assessment of Educational Progress grade 12 preparedness research project judgmental standard setting (JSS) studies: Process report</em>. Prepared for the National Assessment Governing Board. <a href = \"https://www.nagb.gov/content/nagb/assets/documents/what-we-do/preparedness-research/judgmental-standard-setting-studies/Standard_Setting_Process.pdf\">https://www.nagb.gov/content/nagb/assets/documents/what-we-do/preparedness-research/judgmental-standard-setting-studies/Standard_Setting_Process.pdf</a></p>",
"<p>WestEd. (2011b). <em>National Assessment of Educational Progress judgmental standard setting (JSS) studies: Technical Report</em>. Prepared for the National Assessment Governing Board. <a href = \"https://www.nagb.gov/content/nagb/assets/documents/what-we-do/preparedness-research/judgmental-standard-setting-studies/Standard_Setting_Techical.pdf\">https://www.nagb.gov/content/nagb/assets/documents/what-we-do/preparedness-research/judgmental-standard-setting-studies/Standard_Setting_Techical.pdf</a></p>",
"<p>Xi, N., Jerry, L., Freund, D., & Lin, M. (2020). <em>NAEP Grade 12 Academic Preparedness Research: Analyses Relating Michigan Students’ Performance on NAEP to Postsecondary Performance</em>. Educational Testing Service. Prepared for the National Assessment Governing Board. <a href = \"https://www.nagb.gov/focus-areas/reports/preparedness-research/docs/statistical-relationships/landing/analyses-relating-michigan-students-performance.html\">https://www.nagb.gov/focus-areas/reports/preparedness-research/docs/statistical-relationships/landing/analyses-relating-michigan-students-performance.html</a></p>"
]
}